import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.tree import export_text
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import joblib
import os
from datetime import datetime
import warnings
import logging
import json
from typing import Dict, List, Union, Any

# High-performance model imports
import xgboost as xgb

# Advanced pipeline and imbalance handling
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE

# Ignore warnings to keep the output clean
warnings.filterwarnings('ignore')

class AuditLogger:
    """
    Enhanced audit logging system with performance tracking.
    """
    def __init__(self, log_file='destination_model_audit.log'):
        if os.path.exists(log_file):
            os.remove(log_file)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[logging.FileHandler(log_file), logging.StreamHandler()]
        )
        self.logger = logging.getLogger('AuditLogger')

class TravelDestinationPredictor:
    """
    A production-ready travel destination predictor with fixed confusion matrix generation.
    """
    def __init__(self, audit_logger: AuditLogger = None):
        self.audit_logger = audit_logger or AuditLogger()
        self.models: Dict[str, Any] = {}
        self.label_encoders: Dict[str, LabelEncoder] = {}
        self.scalers: Dict[str, StandardScaler] = {}
        self.feature_names: Dict[str, List[str]] = {}
        self.target_encoders: Dict[str, LabelEncoder] = {}
        self.shap_explainer = None
        self.is_trained = False
        
        # FIXED: Properly store all test data variants
        self.X_test_full = None      # Full test set for confusion matrix
        self.y_test_full = None      # Full test labels for confusion matrix
        self.X_test_shap = None      # Smaller sample for SHAP visualizations
        self.y_test_shap = None      # Corresponding labels for SHAP sample
        self.shap_values = None
        self.X_train_sample = None   # Store sample of training data for SHAP
        
        # Dataset statistics for data-driven explanations
        self.dataset_stats = {}

    def _analyze_dataset_patterns(self, train_df: pd.DataFrame, processed_df: pd.DataFrame) -> None:
        """Analyze dataset to extract real patterns and statistics for explanations."""
        print("üîç Analyzing dataset patterns for data-driven explanations...")
        
        try:
            stats = {}
            
            # Destination-specific statistics
            destination_stats = {}
            for destination in train_df['Destination'].unique():
                dest_data = train_df[train_df['Destination'] == destination]
                
                # Convert numeric columns to ensure proper calculations
                duration_clean = pd.to_numeric(dest_data['Duration (days)'], errors='coerce')
                accom_cost_clean = pd.to_numeric(dest_data['Accommodation cost'], errors='coerce')
                transport_cost_clean = pd.to_numeric(dest_data['Transportation cost'], errors='coerce')
                age_clean = pd.to_numeric(dest_data['Traveler age'], errors='coerce')
                
                destination_stats[destination] = {
                    'total_trips': len(dest_data),
                    'avg_duration': duration_clean.mean() if not duration_clean.isna().all() else 0,
                    'duration_range': (duration_clean.min(), duration_clean.max()) if not duration_clean.isna().all() else (0, 0),
                    'avg_accommodation_cost': accom_cost_clean.mean() if not accom_cost_clean.isna().all() else 0,
                    'accommodation_cost_range': (accom_cost_clean.min(), accom_cost_clean.max()) if not accom_cost_clean.isna().all() else (0, 0),
                    'avg_transportation_cost': transport_cost_clean.mean() if not transport_cost_clean.isna().all() else 0,
                    'transportation_cost_range': (transport_cost_clean.min(), transport_cost_clean.max()) if not transport_cost_clean.isna().all() else (0, 0),
                    'avg_total_cost': (accom_cost_clean + transport_cost_clean).mean() if not (accom_cost_clean.isna().all() or transport_cost_clean.isna().all()) else 0,
                    'popular_accommodation_types': dest_data['Accommodation type'].value_counts().to_dict(),
                    'popular_transportation_types': dest_data['Transportation type'].value_counts().to_dict(),
                    'age_demographics': {
                        'avg_age': age_clean.mean() if not age_clean.isna().all() else 0,
                        'age_range': (age_clean.min(), age_clean.max()) if not age_clean.isna().all() else (0, 0),
                        'age_distribution': pd.cut(age_clean.dropna(), 
                                                 bins=[0, 25, 35, 50, 65, 100], 
                                                 labels=['18-25', '26-35', '36-50', '51-65', '65+']).value_counts().to_dict() if not age_clean.isna().all() else {}
                    },
                    'gender_distribution': dest_data['Traveler gender'].value_counts().to_dict(),
                    'nationality_distribution': dest_data['Traveler nationality'].value_counts().to_dict(),
                    'seasonal_patterns': dest_data.groupby(pd.to_datetime(dest_data['Start date'], errors='coerce').dt.month)['Trip ID'].count().to_dict() if not dest_data.empty else {}
                }
            
            stats['destinations'] = destination_stats
            
            # Clean numeric columns for overall analysis
            train_duration_clean = pd.to_numeric(train_df['Duration (days)'], errors='coerce')
            train_accom_clean = pd.to_numeric(train_df['Accommodation cost'], errors='coerce')
            train_transport_clean = pd.to_numeric(train_df['Transportation cost'], errors='coerce')
            train_age_clean = pd.to_numeric(train_df['Traveler age'], errors='coerce')
            
            # Overall dataset patterns
            stats['overall'] = {
                'total_trips': len(train_df),
                'unique_destinations': train_df['Destination'].nunique(),
                'destination_popularity': train_df['Destination'].value_counts().to_dict(),
                'avg_duration_by_destination': train_df.groupby('Destination').apply(
                    lambda x: pd.to_numeric(x['Duration (days)'], errors='coerce').mean()
                ).to_dict(),
                'avg_cost_by_destination': train_df.groupby('Destination').apply(
                    lambda x: (pd.to_numeric(x['Accommodation cost'], errors='coerce') + 
                              pd.to_numeric(x['Transportation cost'], errors='coerce')).mean()
                ).to_dict(),
                'demographic_patterns': {
                    'age_destination_correlation': train_df.groupby('Destination').apply(
                        lambda x: pd.to_numeric(x['Traveler age'], errors='coerce').mean()
                    ).to_dict(),
                    'duration_destination_correlation': train_df.groupby('Destination').apply(
                        lambda x: pd.to_numeric(x['Duration (days)'], errors='coerce').mean()
                    ).to_dict(),
                    'cost_age_correlation': train_df.groupby(pd.cut(train_age_clean.dropna(), 
                                                                   bins=[0, 25, 35, 50, 65, 100], 
                                                                   labels=['18-25', '26-35', '36-50', '51-65', '65+'])).apply(
                        lambda x: (pd.to_numeric(x['Accommodation cost'], errors='coerce') + 
                                  pd.to_numeric(x['Transportation cost'], errors='coerce')).mean()
                    ).to_dict() if not train_age_clean.isna().all() else {}
                }
            }
            
            # Feature correlation insights
            if len(processed_df) > 0:
                numeric_cols = processed_df.select_dtypes(include=[np.number]).columns
                correlations = processed_df[numeric_cols].corr()
                
                stats['feature_insights'] = {
                    'duration_cost_correlation': correlations.loc['Duration (days)', 'Accommodation cost'] if 'Duration (days)' in correlations.index and 'Accommodation cost' in correlations.columns else 0,
                    'age_cost_correlation': correlations.loc['Age', 'Total_Trip_Cost'] if 'Age' in correlations.index and 'Total_Trip_Cost' in correlations.columns else 0,
                    'strong_correlations': {}
                }
                
                # Find strong correlations (>0.5 or <-0.5)
                for col1 in correlations.columns:
                    for col2 in correlations.columns:
                        if col1 != col2 and abs(correlations.loc[col1, col2]) > 0.5:
                            stats['feature_insights']['strong_correlations'][f"{col1}_vs_{col2}"] = correlations.loc[col1, col2]
            
            self.dataset_stats = stats
            print(f"‚úì Analyzed patterns for {len(destination_stats)} destinations with {stats['overall']['total_trips']} total trips")
            
        except Exception as e:
            import traceback
            print(f"‚ùå Error analyzing dataset patterns: {e}")
            print("Full error details:")
            traceback.print_exc()
            print("Using basic dataset statistics instead...")
            self.dataset_stats = {
                'destinations': {},
                'overall': {'total_trips': len(train_df), 'unique_destinations': train_df['Destination'].nunique()},
                'feature_insights': {}
            }

    def _calculate_derived_features(self, data: Union[pd.DataFrame, Dict[str, Any]]) -> pd.DataFrame:
        """Calculate derived features from raw data."""
        if isinstance(data, dict):
            data = pd.DataFrame([data])
        
        # Handle age calculation
        if 'Traveler DOB' in data.columns:
            data['Traveler DOB'] = pd.to_datetime(data['Traveler DOB'], format='%m/%d/%Y', errors='coerce')
            data['Age'] = (pd.to_datetime(datetime.now()) - data['Traveler DOB']).dt.days / 365.25
        elif 'Traveler age' in data.columns:
            data.rename(columns={'Traveler age': 'Age'}, inplace=True)
        else:
            data['Age'] = 35
        
        # Create age groups
        if 'Age' in data.columns:
            data['Age_Group'] = pd.cut(data['Age'], bins=[0, 25, 35, 50, 65, 100], 
                                     labels=['18-25', '26-35', '36-50', '51-65', '65+'], right=False)
        
        # Budget categories
        data['Accommodation_Budget'] = pd.cut(data['Accommodation cost'], 
                                            bins=[0, 500, 1500, float('inf')], 
                                            labels=['Budget', 'Mid_Range', 'Luxury'])
        data['Transportation_Budget'] = pd.cut(data['Transportation cost'], 
                                             bins=[0, 400, 1000, float('inf')], 
                                             labels=['Budget', 'Mid_Range', 'Premium'])
        
        # Cost calculations
        data['Cost_Per_Day'] = data['Accommodation cost'] / data['Duration (days)'].replace(0, 1)
        data['Total_Trip_Cost'] = data['Accommodation cost'] + data['Transportation cost']
        
        return data

    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Preprocess raw data for model consumption."""
        # Clean cost columns
        cost_cols = ['Accommodation cost', 'Transportation cost']
        for col in cost_cols:
            if col in df.columns and df[col].dtype == 'object':
                df[col] = df[col].astype(str).str.replace(r'[$,]', '', regex=True)
        
        # Convert to numeric and handle missing values
        numeric_cols = ['Accommodation cost', 'Transportation cost', 'Duration (days)', 'Traveler age']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(df[col].median())
        
        # Calculate derived features
        df = self._calculate_derived_features(df)
        return df

    def prepare_features(self, data: pd.DataFrame, is_fitting: bool, task_name: str = 'destination_prediction') -> tuple:
        """Prepare features for training or prediction."""
        if is_fitting:
            exclude_columns = ['Trip ID', 'Destination', 'Start date', 'End date', 'Traveler name', 'Traveler DOB']
            self.feature_names[task_name] = [col for col in data.columns if col not in exclude_columns]
            self.label_encoders[task_name] = {}
        
        X = data.reindex(columns=self.feature_names[task_name]).copy()
        
        y_encoded = None
        if 'Destination' in data.columns:
            y = data['Destination'].str.strip().copy()
            if is_fitting:
                self.target_encoders[task_name] = LabelEncoder()
                y_encoded = self.target_encoders[task_name].fit_transform(y)
            else:
                y_encoded = self.target_encoders[task_name].transform(y)

        # Handle categorical columns
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
        for col in categorical_cols:
            X[col] = X[col].astype(str)
            if is_fitting:
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col])
                self.label_encoders[task_name][col] = le
            else:
                le = self.label_encoders[task_name].get(col)
                if le:
                    known_labels = list(le.classes_)
                    X[col] = X[col].apply(lambda x: x if x in known_labels else 'nan')
                    if 'nan' not in le.classes_:
                        le.classes_ = np.append(le.classes_, 'nan')
                    X[col] = le.transform(X[col])
        
        # Handle numerical columns
        numerical_cols = X.select_dtypes(include=np.number).columns.tolist()
        if numerical_cols:
            X[numerical_cols] = X[numerical_cols].fillna(0)
            if is_fitting:
                self.scalers[task_name] = StandardScaler()
                X[numerical_cols] = self.scalers[task_name].fit_transform(X[numerical_cols])
            else:
                if task_name in self.scalers:
                    X[numerical_cols] = self.scalers[task_name].transform(X[numerical_cols])
        
        return X, y_encoded

    def train_model(self, data_path: str = 'extended_travel_trips.csv', task_name: str = 'destination_prediction') -> Dict[str, Any]:
        """Train the model using the provided dataset."""
        try:
            raw_data = pd.read_csv(data_path)
        except FileNotFoundError:
            raise FileNotFoundError(f"Training data file '{data_path}' not found.")
        print("--- Starting Model Training ---")
        
        # Data cleaning and preparation
        raw_data.dropna(subset=['Destination'], inplace=True)
        initial_rows = len(raw_data)
        raw_data = raw_data[raw_data['Destination'].str.contains(',', na=False)].copy()
        
        # Standardize destination names
        consolidation_map = {
            'Bangkok, Thai': 'Bangkok, Thailand',
            'Cape Town, SA': 'Cape Town, South Africa',
            'New York City, USA': 'New York, USA',
            'London, UK': 'London, United Kingdom',
            'Sydney, AUS': 'Sydney, Australia',
            'Sydney, Aus': 'Sydney, Australia',
            'Phuket, Thai': 'Phuket, Thailand'
        }
        raw_data['Destination'] = raw_data['Destination'].replace(consolidation_map)
        
        print(f"Data cleaned: {len(raw_data)} valid samples from {initial_rows} total")
        
        # Temporal validation split
        raw_data['Start date'] = pd.to_datetime(raw_data['Start date'], errors='coerce')
        raw_data.dropna(subset=['Start date'], inplace=True)
        raw_data = raw_data.sort_values(by='Start date').reset_index(drop=True)
        
        split_index = int(len(raw_data) * 0.8)
        train_df = raw_data.iloc[:split_index]
        test_df = raw_data.iloc[split_index:]
        
        print(f"Training on {len(train_df)} samples, testing on {len(test_df)} samples")
        
        # Preprocess data
        train_processed = self.preprocess_data(train_df.copy())
        test_processed = self.preprocess_data(test_df.copy())
        
        # Analyze dataset patterns for data-driven explanations
        self._analyze_dataset_patterns(train_df, train_processed)
        
        # Prepare features
        X_train, y_train = self.prepare_features(train_processed.copy(), is_fitting=True, task_name=task_name)
        X_test, y_test = self.prepare_features(test_processed.copy(), is_fitting=False, task_name=task_name)
        
        # FIXED: Store the FULL test data for confusion matrix generation
        self.X_test_full = X_test.copy()
        self.y_test_full = y_test.copy()
        
        print(f"‚úì Stored full test set: {len(self.X_test_full)} samples for confusion matrix")
        
        # Store a sample of training data for SHAP background
        sample_size = min(100, len(X_train))
        self.X_train_sample = X_train.sample(n=sample_size, random_state=42).copy()
        
        # Configure XGBoost model
        xgb_model = xgb.XGBClassifier(
            n_jobs=-1, 
            eval_metric='mlogloss', 
            random_state=42,
            enable_categorical=False
        )
        
        # Hyperparameter tuning
        param_grid = {
            'classifier__n_estimators': [100, 200],
            'classifier__learning_rate': [0.05, 0.1],
            'classifier__max_depth': [3, 5]
        }
        pipeline = ImbPipeline(steps=[
            ('smote', SMOTE(random_state=42)),
            ('classifier', xgb_model)
        ])
        cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)
        random_search = RandomizedSearchCV(
            estimator=pipeline,
            param_distributions=param_grid,
            n_iter=2,
            cv=cv,
            scoring='accuracy',
            n_jobs=-1,
            random_state=42,
            verbose=1
        )
        print("Training model with hyperparameter tuning...")
        random_search.fit(X_train, y_train)
        best_model = random_search.best_estimator_
        
        # Use FULL test set for final evaluation
        test_accuracy = best_model.score(self.X_test_full, self.y_test_full)
        train_accuracy = best_model.score(X_train, y_train)
        
        # Store model
        self.models[task_name] = {
            'model': best_model, 
            'name': 'XGBoost_Tuned_SMOTE', 
            'test_accuracy': test_accuracy,
            'train_accuracy': train_accuracy,
            'best_params': random_search.best_params_
        }
        
        # Create SHAP explainer
        self._create_shap_explainer(best_model.named_steps['classifier'], self.X_train_sample)
        
        # FIXED: Create a separate smaller sample for SHAP visualizations only
        if self.shap_explainer:
            try:
                print("Calculating SHAP values for visualizations...")
                # Use a smaller sample ONLY for SHAP calculations
                test_sample_size = min(50, len(self.X_test_full))
                test_sample_indices = np.random.choice(len(self.X_test_full), test_sample_size, replace=False)
                self.X_test_shap = self.X_test_full.iloc[test_sample_indices].copy()
                self.y_test_shap = self.y_test_full[test_sample_indices].copy()
                
                self.shap_values = self.shap_explainer.shap_values(self.X_test_shap)
                print(f"‚úì SHAP values calculated for {len(self.X_test_shap)} samples")
            except Exception as e:
                print(f"Warning: Could not calculate SHAP values: {e}")
                self.shap_values = None
                self.X_test_shap = None
                self.y_test_shap = None
        
        self.is_trained = True
        
        # Generate predictions for evaluation using FULL test set for metrics
        y_pred_full = best_model.predict(self.X_test_full)
        
        # Calculate additional metrics using FULL test set
        report = classification_report(self.y_test_full, y_pred_full, 
                                    target_names=self.target_encoders[task_name].classes_, 
                                    zero_division=0, output_dict=True)
        
        training_results = {
            "status": "success",
            "model_name": self.models[task_name]['name'],
            "test_accuracy": round(test_accuracy, 4),
            "train_accuracy": round(train_accuracy, 4),
            "best_parameters": random_search.best_params_,
            "training_samples": len(X_train),
            "test_samples": len(self.X_test_full),
            "test_samples_shap": len(self.X_test_shap) if self.X_test_shap is not None else 0,
            "unique_destinations": len(self.target_encoders[task_name].classes_),
            "feature_count": len(self.feature_names[task_name]),
            "classification_report": report,
            "training_timestamp": datetime.now().isoformat()
        }
        
        print(f"‚úì Model training completed successfully!")
        print(f"‚úì Test Accuracy: {test_accuracy:.4f}")
        print(f"‚úì Train Accuracy: {train_accuracy:.4f}")
        print(f"‚úì Full test set size: {len(self.X_test_full)} (for confusion matrix)")
        print(f"‚úì SHAP sample size: {len(self.X_test_shap) if self.X_test_shap is not None else 0} (for visualizations)")
        
        return training_results

    def _create_shap_explainer(self, model: Any, X_background: pd.DataFrame) -> None:
        """Create SHAP explainer for model interpretability."""
        try:
            # Use a smaller background dataset for TreeExplainer
            background_size = min(50, len(X_background))
            background_sample = X_background.sample(n=background_size, random_state=42)
            self.shap_explainer = shap.TreeExplainer(model, background_sample)
            print("‚úì SHAP explainer created successfully")
        except Exception as e:
            print(f"Warning: Could not create SHAP explainer: {e}")
            self.shap_explainer = None

    def _ensure_shap_ready(self):
        """Ensure SHAP explainer and values are available."""
        if not self.is_trained:
            raise ValueError("Model must be trained first")
        
        if self.shap_explainer is None:
            raise ValueError("SHAP explainer not available")
        
        if self.shap_values is None or self.X_test_shap is None:
            print("Calculating SHAP values...")
            try:
                if self.X_test_full is not None:
                    test_sample_size = min(50, len(self.X_test_full))
                    self.X_test_shap = self.X_test_full.sample(n=test_sample_size, random_state=42)
                    self.y_test_shap = self.y_test_full[:len(self.X_test_shap)]
                else:
                    raise ValueError("No test data available")
                
                self.shap_values = self.shap_explainer.shap_values(self.X_test_shap)
                print("‚úì SHAP values calculated successfully")
            except Exception as e:
                raise ValueError(f"Could not calculate SHAP values: {e}")

    def generate_confusion_matrix(self, save_path: str = None, figsize: tuple = (16, 12)) -> None:
        """
        Generate and display confusion matrix for the trained model using FULL test set.
        FIXED: Now properly uses the full test dataset stored during training.
        
        Args:
            save_path: Path to save the confusion matrix plot (optional)
            figsize: Figure size tuple
        """
        if not self.is_trained:
            print("‚ùå Error: Model must be trained first")
            return
        
        # FIXED: Always use the full test set that was stored during training
        if self.X_test_full is None or self.y_test_full is None:
            print("‚ùå Error: Full test data not available. Please retrain the model.")
            return
        
        try:
            print(f"üîÑ Generating confusion matrix using {len(self.X_test_full)} test samples...")
            
            model = self.models['destination_prediction']['model']
            
            # Generate predictions on the FULL test set
            y_pred = model.predict(self.X_test_full)
            y_true = self.y_test_full
            
            # Get class names
            class_names = self.target_encoders['destination_prediction'].classes_
            
            # Calculate confusion matrix
            cm = confusion_matrix(y_true, y_pred)
            
            print(f"‚úì Confusion matrix calculated:")
            print(f"  - Matrix shape: {cm.shape}")
            print(f"  - Total predictions: {cm.sum()}")
            print(f"  - Number of classes: {len(class_names)}")
            print(f"  - Correct predictions: {np.trace(cm)}")
            print(f"  - Overall accuracy: {np.trace(cm) / cm.sum():.4f}")
            
            # Create the plot
            plt.figure(figsize=figsize)
            
            # Shorten class names for better display
            short_names = [name.split(',')[0] if ',' in name else name for name in class_names]
            
            # Create heatmap with improved styling
            ax = sns.heatmap(cm, 
                            annot=True, 
                            fmt='d', 
                            cmap='Blues',
                            xticklabels=short_names,
                            yticklabels=short_names,
                            cbar_kws={'label': 'Number of Predictions', 'shrink': 0.8},
                            square=True,
                            linewidths=0.5,
                            annot_kws={'size': 8})
            
            plt.title('Confusion Matrix - Travel Destination Predictions', 
                     fontsize=20, fontweight='bold', pad=30)
            plt.xlabel('Predicted Destination', fontsize=16, fontweight='bold', labelpad=15)
            plt.ylabel('True Destination', fontsize=16, fontweight='bold', labelpad=15)
            
            # Improve label formatting - less rotation for better readability
            plt.xticks(rotation=30, ha='right', fontsize=12, fontweight='medium')
            plt.yticks(rotation=0, fontsize=12, fontweight='medium')
            
            # Add text annotation with summary stats
            accuracy = np.trace(cm) / cm.sum()
            plt.figtext(0.02, 0.02, f'Overall Accuracy: {accuracy:.3f} | Total Samples: {cm.sum()}', 
                       fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"‚úì Confusion matrix saved to: {save_path}")
            
            plt.show()
            
            # Print detailed per-class statistics
            print(f"\nüìä Detailed Classification Statistics:")
            print("="*60)
            for i, class_name in enumerate(class_names):
                class_total = cm[i].sum()
                class_correct = cm[i, i]
                if class_total > 0:
                    precision = class_correct / class_total
                    # Calculate recall (how many of actual class were predicted correctly)
                    recall = class_correct / cm[:, i].sum() if cm[:, i].sum() > 0 else 0
                    print(f"{class_name:25} | Correct: {class_correct:2d}/{class_total:2d} | "
                          f"Precision: {precision:.3f} | Recall: {recall:.3f}")
            print("="*60)
            
        except Exception as e:
            print(f"‚ùå Error generating confusion matrix: {e}")
            import traceback
            traceback.print_exc()

    def generate_dependence_plot(self, feature_name: str, interaction_feature: str = None, 
                           save_path: str = None, figsize: tuple = (12, 8)) -> None:
        """Generate SHAP dependence plot for a specific feature."""
        self._ensure_shap_ready()
        
        # Get feature names
        feature_names = self.feature_names.get('destination_prediction', [])
        if not feature_names:
            raise ValueError("Feature names not available")
        
        # Check if feature exists
        if feature_name not in feature_names:
            print(f"Available features: {feature_names}")
            raise ValueError(f"Feature '{feature_name}' not found in model features")
        
        try:
            # Set up the plot
            plt.figure(figsize=figsize)
            
            # Handle different SHAP value formats
            shap_values_to_plot = self.shap_values
            
            # For multi-class classification, SHAP returns a list or 3D array
            if isinstance(shap_values_to_plot, list):
                # Use the first class for visualization
                shap_values_to_plot = shap_values_to_plot[0]
                class_name = self.target_encoders['destination_prediction'].classes_[0]
                print(f"Using SHAP values for class: {class_name}")
            elif len(shap_values_to_plot.shape) == 3:
                # Shape: (n_samples, n_features, n_classes)
                shap_values_to_plot = shap_values_to_plot[:, :, 0]
                class_name = self.target_encoders['destination_prediction'].classes_[0]
                print(f"Using SHAP values for class: {class_name}")
            
            # Create the dependence plot
            shap.dependence_plot(
                feature_name,
                shap_values_to_plot,
                self.X_test_shap,
                feature_names=feature_names,
                interaction_index=interaction_feature,
                show=False
            )
            
            plt.title(f'SHAP Dependence Plot: {feature_name.replace("_", " ").title()}', 
                    fontsize=16, fontweight='bold', pad=20)
            plt.xlabel(f'{feature_name.replace("_", " ").title()} Value', fontsize=12)
            plt.ylabel(f'SHAP Value for {feature_name.replace("_", " ").title()}', fontsize=12)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"Dependence plot saved to: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"Error generating dependence plot: {e}")
            print("Attempting manual implementation...")
            self._manual_dependence_plot(feature_name, figsize, save_path)

    def _manual_dependence_plot(self, feature_name: str, figsize: tuple, save_path: str = None):
        """Manual implementation of dependence plot."""
        try:
            feature_names = self.feature_names['destination_prediction']
            feature_idx = feature_names.index(feature_name)
            
            # Get feature values and SHAP values
            feature_values = self.X_test_shap.iloc[:, feature_idx].values
            
            # Handle SHAP values format
            shap_vals = self.shap_values
            if isinstance(shap_vals, list):
                shap_feature_values = shap_vals[0][:, feature_idx]
            elif len(shap_vals.shape) == 3:
                shap_feature_values = shap_vals[:, feature_idx, 0]
            else:
                shap_feature_values = shap_vals[:, feature_idx]
            
            plt.figure(figsize=figsize)
            scatter = plt.scatter(feature_values, shap_feature_values, 
                                alpha=0.6, c=feature_values, cmap='viridis', s=50)
            plt.colorbar(scatter, label=f'{feature_name} Value')
            plt.xlabel(f'{feature_name.replace("_", " ").title()}', fontsize=12)
            plt.ylabel(f'SHAP Value', fontsize=12)
            plt.title(f'SHAP Dependence Plot: {feature_name.replace("_", " ").title()}', 
                     fontsize=16, fontweight='bold')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"Manual dependence plot saved to: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"Manual dependence plot also failed: {e}")

    def generate_decision_plot(self, sample_indices: List[int] = None, max_samples: int = 20, 
                             save_path: str = None, figsize: tuple = (14, 10)) -> None:
        """Generate SHAP decision plot showing cumulative SHAP values with improved x-axis spacing."""
        import numpy as np
        self._ensure_shap_ready()
        
        try:
            # Determine which samples to plot
            if sample_indices is None:
                n_samples = min(max_samples, len(self.X_test_shap))
                sample_indices = np.random.choice(len(self.X_test_shap), n_samples, replace=False)
            
            # Handle different SHAP value formats
            shap_values = self.shap_values
            expected_value = self.shap_explainer.expected_value
            
            if isinstance(shap_values, list):
                # Multi-class case - use first class
                shap_values_to_plot = shap_values[0][sample_indices]
                expected_value_to_plot = expected_value[0] if hasattr(expected_value, '__len__') else expected_value
                class_name = self.target_encoders['destination_prediction'].classes_[0]
            elif len(shap_values.shape) == 3:
                # Shape: (n_samples, n_features, n_classes)
                shap_values_to_plot = shap_values[sample_indices, :, 0]
                expected_value_to_plot = expected_value[0] if hasattr(expected_value, '__len__') else expected_value
                class_name = self.target_encoders['destination_prediction'].classes_[0]
            else:
                # Binary classification or regression
                shap_values_to_plot = shap_values[sample_indices]
                expected_value_to_plot = expected_value
                class_name = "Prediction"
            
            plt.figure(figsize=figsize)
            
            # Create decision plot
            shap.decision_plot(
                expected_value_to_plot,
                shap_values_to_plot,
                self.X_test_shap.iloc[sample_indices],
                feature_names=self.feature_names['destination_prediction'],
                show=False,
                link='logit'
            )
            
            # FIXED: Adjust x-axis limits and set reasonable tick spacing
            current_xlim = plt.xlim()
            x_min, x_max = current_xlim
            x_range = x_max - x_min
            
            # Set x-axis with 0.2 spacing and remove negative values
            tick_spacing = 0.2
            
            # Start from 0 (remove negative values) and extend to x_max + 0.05
            start_tick = 0.0
            end_tick = np.ceil((x_max + 0.05) / tick_spacing) * tick_spacing
            x_ticks = np.arange(start_tick, end_tick + tick_spacing, tick_spacing)
            
            # Set x-axis limits to start from 0 and extend by 0.05
            plt.xlim(0, x_max + 0.05)
            
            plt.xticks(x_ticks)
            plt.xticks(rotation=45, fontsize=12)  # Rotate labels to prevent overlap
            
            plt.title(f'SHAP Decision Plot: Model Decision Process\n({len(sample_indices)} samples, Class: {class_name})', 
                     fontsize=18, fontweight='bold', pad=30)
            plt.xlabel('Model Output Value (Zoomed View)', fontsize=14, fontweight='medium', labelpad=15)
            plt.ylabel('Features', fontsize=14, fontweight='medium', labelpad=15)
            
            # Add grid for better readability
            plt.grid(True, alpha=0.3, axis='x')
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"Decision plot saved to: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"Error generating decision plot: {e}")
            print("Attempting manual implementation...")
            self._manual_decision_plot(sample_indices, figsize, save_path)

    def _manual_decision_plot(self, sample_indices: List[int], figsize: tuple, save_path: str = None):
        """Manual implementation of decision plot."""
        try:
            # Get SHAP values and feature names
            shap_vals = self.shap_values
            feature_names = self.feature_names['destination_prediction']
            
            if isinstance(shap_vals, list):
                shap_values_sample = shap_vals[0][sample_indices]
            elif len(shap_vals.shape) == 3:
                shap_values_sample = shap_vals[sample_indices, :, 0]
            else:
                shap_values_sample = shap_vals[sample_indices]
            
            # Calculate cumulative SHAP values
            expected_value = self.shap_explainer.expected_value
            if hasattr(expected_value, '__len__'):
                expected_value = expected_value[0]
            
            cumulative_shap = np.cumsum(shap_values_sample, axis=1)
            
            plt.figure(figsize=figsize)
            
            # Plot each sample's decision path
            colors = plt.cm.viridis(np.linspace(0, 1, min(10, len(sample_indices))))
            for i, sample_idx in enumerate(sample_indices[:min(10, len(sample_indices))]):
                plt.plot(range(len(feature_names)), 
                        expected_value + cumulative_shap[i], 
                        alpha=0.7, linewidth=2, color=colors[i],
                        label=f'Sample {sample_idx}')
            
            # Add expected value line
            plt.axhline(y=expected_value, color='red', linestyle='--', 
                       alpha=0.8, label='Expected Value')
            
            # FIXED: Adjust x-axis limits by +0.05 and set tick spacing to 0.05
            current_xlim = plt.xlim()
            plt.xlim(current_xlim[0], current_xlim[1] + 0.05)
            
            # Set x-axis ticks with 0.05 spacing
            x_min, x_max = plt.xlim()
            x_ticks = np.arange(0, x_max + 0.05, 0.05)
            plt.xticks(x_ticks)
            
            plt.xlabel('Feature Index', fontsize=12)
            plt.ylabel('Cumulative SHAP Value', fontsize=12)
            plt.title('Manual Decision Plot: Cumulative SHAP Values', fontsize=16, fontweight='bold')
            plt.xticks(range(len(feature_names)), 
                      [name.replace('_', ' ')[:15] + '...' if len(name) > 15 else name.replace('_', ' ') 
                       for name in feature_names], 
                      rotation=45, ha='right')
            plt.grid(True, alpha=0.3)
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"Manual decision plot saved to: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"Manual decision plot also failed: {e}")

    def generate_waterfall_plot(self, sample_index: int = 0, class_index: int = 0, 
                               save_path: str = None, figsize: tuple = (12, 8)) -> None:
        """Generate SHAP waterfall plot for a single prediction."""
        self._ensure_shap_ready()
        
        if sample_index >= len(self.X_test_shap):
            print(f"Error: Sample index {sample_index} out of range. Max index: {len(self.X_test_shap)-1}")
            return
        
        try:
            plt.figure(figsize=figsize)
            
            # Get SHAP values for the specific sample and class
            if isinstance(self.shap_values, list):
                sample_shap_values = self.shap_values[class_index][sample_index]
                expected_value = self.shap_explainer.expected_value[class_index]
            elif len(self.shap_values.shape) == 3:
                sample_shap_values = self.shap_values[sample_index, :, class_index]
                expected_value = self.shap_explainer.expected_value[class_index]
            else:
                sample_shap_values = self.shap_values[sample_index]
                expected_value = self.shap_explainer.expected_value
            
            # Create waterfall plot
            shap.waterfall_plot(
                shap.Explanation(
                    values=sample_shap_values,
                    base_values=expected_value,
                    data=self.X_test_shap.iloc[sample_index].values,
                    feature_names=self.feature_names['destination_prediction']
                ),
                show=False
            )
            
            plt.title(f'SHAP Waterfall Plot: Sample {sample_index}', 
                     fontsize=18, fontweight='bold', pad=25)
            
            # Improve axis labels
            plt.xlabel('Feature Contribution to Prediction', fontsize=14, fontweight='medium')
            
            # Add subtle grid
            plt.grid(True, alpha=0.2, axis='x')
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight', 
                           facecolor='white', edgecolor='none')
                print(f"Waterfall plot saved to: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"Error generating waterfall plot: {e}")

    def generate_summary_plots(self, save_dir: str = None):
        """Generate both bar and dot summary plots with improved x-axis spacing."""
        self._ensure_shap_ready()
        
        # Bar plot
        try:
            plt.figure(figsize=(12, 8))
            
            shap_vals = self.shap_values
            if isinstance(shap_vals, list):
                shap_vals = shap_vals[0]  # Use first class
            elif len(shap_vals.shape) == 3:
                shap_vals = shap_vals[:, :, 0]  # Use first class
            
            shap.summary_plot(
                shap_vals, 
                self.X_test_shap,
                feature_names=self.feature_names['destination_prediction'],
                plot_type="bar",
                show=False
            )
            
            # FIXED: Adjust x-axis limits and set reasonable tick spacing
            current_xlim = plt.xlim()
            x_min, x_max = current_xlim
            x_range = x_max - x_min
            
            # Extend x-axis by 0.05
            plt.xlim(x_min, x_max + 0.05)
            
            # Set reasonable tick spacing based on the range
            if x_range <= 0.5:
                tick_spacing = 0.05
            elif x_range <= 1:
                tick_spacing = 0.1
            elif x_range <= 2:
                tick_spacing = 0.2
            elif x_range <= 5:
                tick_spacing = 0.5
            else:
                tick_spacing = 1.0
            
            # Generate ticks with proper spacing
            start_tick = np.floor(x_min / tick_spacing) * tick_spacing
            end_tick = np.ceil((x_max + 0.05) / tick_spacing) * tick_spacing
            x_ticks = np.arange(start_tick, end_tick + tick_spacing, tick_spacing)
            plt.xticks(x_ticks)
            
            plt.title('SHAP Summary Plot (Feature Importance)', 
                     fontsize=18, fontweight='bold', pad=25)
            plt.xlabel('Mean(|SHAP value|) (Average Impact on Model Output)', 
                      fontsize=14, fontweight='medium', labelpad=15)
            plt.ylabel('Features', fontsize=14, fontweight='medium', labelpad=15)
            
            # Improve grid and aesthetics
            plt.grid(True, alpha=0.3, axis='x')
            
            plt.tight_layout()
            
            if save_dir:
                plt.savefig(f"{save_dir}/summary_bar.png", dpi=300, bbox_inches='tight', 
                           facecolor='white', edgecolor='none')
            plt.show()
            
        except Exception as e:
            print(f"Error generating bar summary plot: {e}")
        
        # Dot plot
        try:
            plt.figure(figsize=(12, 8))
            
            shap.summary_plot(
                shap_vals, 
                self.X_test_shap,
                feature_names=self.feature_names['destination_prediction'],
                show=False
            )
            plt.title('SHAP Summary Plot (Feature Impact)', 
                     fontsize=18, fontweight='bold', pad=25)
            plt.xlabel('SHAP Value (Impact on Model Output)', 
                      fontsize=14, fontweight='medium', labelpad=15)
            plt.ylabel('Features', fontsize=14, fontweight='medium', labelpad=15)
            
            # Improve grid and aesthetics
            plt.grid(True, alpha=0.3, axis='x')
            
            plt.tight_layout()
            
            if save_dir:
                plt.savefig(f"{save_dir}/summary_dot.png", dpi=300, bbox_inches='tight',
                           facecolor='white', edgecolor='none')
            plt.show()
            
        except Exception as e:
            print(f"Error generating dot summary plot: {e}")

    def generate_all_shap_plots(self, output_dir: str = "shap_plots") -> None:
        """
        Generate all SHAP visualization plots with improved error handling.
        
        Args:
            output_dir: Directory to save all plots
        """
        if not self.is_trained:
            print("Error: Model must be trained first")
            return
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        print("Generating comprehensive SHAP visualizations...")
        
        # 1. Summary plots
        try:
            print("Generating summary plots...")
            self.generate_summary_plots(save_dir=output_dir)
            print("‚úì Summary plots generated")
        except Exception as e:
            print(f"‚úó Error generating summary plots: {e}")
        
        # 2. Dependence plots for top features
        try:
            print("Generating dependence plots...")
            important_features = self.feature_names['destination_prediction'][:3]  # Top 3 features
            for i, feature in enumerate(important_features):
                try:
                    self.generate_dependence_plot(
                        feature, 
                        save_path=os.path.join(output_dir, f"dependence_plot_{feature.replace(' ', '_')}.png")
                    )
                except Exception as fe:
                    print(f"  - Failed to generate dependence plot for {feature}: {fe}")
            print("‚úì Dependence plots generated")
        except Exception as e:
            print(f"‚úó Error generating dependence plots: {e}")
        
        # 3. Decision plot
        try:
            print("Generating decision plot...")
            self.generate_decision_plot(
                max_samples=15,
                save_path=os.path.join(output_dir, "decision_plot.png")
            )
            print("‚úì Decision plot generated")
        except Exception as e:
            print(f"‚úó Error generating decision plot: {e}")
        
        # 4. Waterfall plots for first few samples
        try:
            print("Generating waterfall plots...")
            for i in range(min(3, len(self.X_test_shap) if self.X_test_shap is not None else 0)):
                try:
                    self.generate_waterfall_plot(
                        sample_index=i,
                        save_path=os.path.join(output_dir, f"waterfall_plot_sample_{i}.png")
                    )
                except Exception as we:
                    print(f"  - Failed to generate waterfall plot for sample {i}: {we}")
            print("‚úì Waterfall plots generated")
        except Exception as e:
            print(f"‚úó Error generating waterfall plots: {e}")
        
        print(f"SHAP visualizations saved to: {output_dir}")

    def generate_classification_report_plot(self, save_path: str = None, figsize: tuple = (12, 8)) -> None:
        """
        Generate a visual classification report heatmap using FULL test set.
        
        Args:
            save_path: Path to save the plot (optional)
            figsize: Figure size tuple
        """
        if not self.is_trained:
            print("Error: Model must be trained first")
            return
        
        if self.X_test_full is None or self.y_test_full is None:
            print("Error: Full test data not available")
            return
        
        try:
            model = self.models['destination_prediction']['model']
            y_pred = model.predict(self.X_test_full)
            y_true = self.y_test_full
            
            class_names = self.target_encoders['destination_prediction'].classes_
            
            # Generate classification report
            report = classification_report(y_true, y_pred, 
                                         target_names=class_names,
                                         output_dict=True,
                                         zero_division=0)
            
            # Convert to DataFrame for visualization
            df_report = pd.DataFrame(report).T
            
            # Remove the 'support' column for cleaner visualization
            if 'support' in df_report.columns:
                df_report = df_report.drop(columns=['support'])
            
            # Create the heatmap
            plt.figure(figsize=figsize)
            
            sns.heatmap(df_report.iloc[:-3], # Exclude macro/weighted avg rows
                       annot=True, 
                       fmt='.3f',
                       cmap='RdYlBu_r',
                       center=0.5,
                       cbar_kws={'label': 'Score'})
            
            plt.title('Classification Report Heatmap', fontsize=16, fontweight='bold', pad=20)
            plt.xlabel('Metrics', fontsize=12)
            plt.ylabel('Destinations', fontsize=12)
            plt.xticks(rotation=0)
            plt.yticks(rotation=0)
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"Classification report plot saved to: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"Error generating classification report plot: {e}")

    def generate_model_performance_plots(self, output_dir: str = "performance_plots") -> None:
        """
        Generate comprehensive model performance visualizations using FULL test set.
        
        Args:
            output_dir: Directory to save all performance plots
        """
        if not self.is_trained:
            print("Error: Model must be trained first")
            return
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        print("Generating model performance visualizations...")
        
        # 1. Confusion Matrix (FIXED - uses full test set)
        try:
            print("Generating confusion matrix...")
            self.generate_confusion_matrix(
                save_path=os.path.join(output_dir, "confusion_matrix.png")
            )
            print("‚úì Confusion matrix generated")
        except Exception as e:
            print(f"‚úó Error generating confusion matrix: {e}")
        
        # 2. Classification Report Heatmap
        try:
            print("Generating classification report...")
            self.generate_classification_report_plot(
                save_path=os.path.join(output_dir, "classification_report.png")
            )
            print("‚úì Classification report generated")
        except Exception as e:
            print(f"‚úó Error generating classification report: {e}")
        
        print(f"Performance visualizations saved to: {output_dir}")

    def _get_shap_explanation_data(self, X_user: pd.DataFrame, user_original_values: pd.DataFrame, 
                                  class_index: int, task_name: str, actual_probability: float) -> Dict[str, Any]:
        """Generate comprehensive SHAP-based explanations for predictions."""
        if self.shap_explainer is None: 
            return {}
        
        try:
            shap_values_matrix = self.shap_explainer.shap_values(X_user)
            
            base_value = self.shap_explainer.expected_value
            if hasattr(base_value, '__len__') and not isinstance(base_value, str):
                base_value = base_value[class_index]

            if isinstance(shap_values_matrix, list):
                shap_values_for_class = shap_values_matrix[class_index][0]
            else:
                shap_values_for_class = shap_values_matrix[0, :, class_index]

            feature_impact = pd.Series(shap_values_for_class, index=self.feature_names[task_name])
            
            # Get more features for detailed explanation (top 3 positive, top 2 negative)
            positive_contributors = feature_impact[feature_impact > 0].nlargest(3)
            negative_contributors = feature_impact[feature_impact < 0].nsmallest(2)
            
            all_contributors = pd.concat([positive_contributors, negative_contributors])

            explanation_details = {
                "final_probability": float(actual_probability),
                "base_value": float(base_value),
                "contributions": [],
                "feature_impacts": feature_impact.to_dict()  # All feature impacts for context
            }
            
            for feature, contribution_value in all_contributors.items():
                try:
                    user_value = user_original_values[feature].iloc[0]
                    original_value = user_value  # Store original for analysis
                    
                    # Format values appropriately
                    if isinstance(user_value, (int, float)):
                        if 'Cost' in feature:
                            user_value = f"${user_value:,.0f}"
                        elif 'Age' in feature:
                            user_value = f"{user_value:.0f} years old"
                        elif 'Duration' in feature:
                            user_value = f"{user_value:.0f} days"
                        else:
                            user_value = f"{user_value:.1f}"

                    explanation_details["contributions"].append({
                        "feature": feature, 
                        "contribution": float(contribution_value),
                        "user_value": str(user_value),
                        "raw_value": original_value,
                        "impact_strength": "Strong" if abs(contribution_value) > 0.1 else "Moderate" if abs(contribution_value) > 0.05 else "Mild"
                    })
                except Exception as feature_error:
                    # Skip this feature if there's an issue
                    continue
            
            return explanation_details
        except Exception as e:
            print(f"Warning: Could not generate SHAP explanation: {e}")
            return {}

    def _get_feature_context(self, feature_name: str, value: Any, contribution: float) -> str:
        """Provide context about what a feature value means."""
        feature_lower = feature_name.lower()
        
        # Age context
        if 'age' in feature_lower:
            age = float(value) if isinstance(value, (int, float)) else 25
            if age < 25:
                return "younger travelers like yourself often prefer"
            elif age < 35:
                return "travelers in their late twenties and early thirties typically enjoy"  
            elif age < 50:
                return "mid-career travelers often seek"
            else:
                return "experienced travelers with established preferences usually prefer"
        
        # Duration context
        if 'duration' in feature_lower:
            days = float(value) if isinstance(value, (int, float)) else 7
            if days <= 3:
                return "short getaways like yours work well for"
            elif days <= 7:
                return "week-long trips are perfect for"
            elif days <= 14:
                return "extended vacations allow you to fully experience"
            else:
                return "long-term stays give you deep immersion in"
        
        # Cost context
        if 'cost' in feature_lower:
            cost = float(value) if isinstance(value, (int, float)) else 1000
            if 'accommodation' in feature_lower:
                if cost < 500:
                    return "budget-friendly accommodation choices indicate you prefer"
                elif cost < 1500:
                    return "mid-range accommodation budgets typically suit"
                else:
                    return "premium accommodation preferences align with"
            elif 'transportation' in feature_lower:
                if cost < 400:
                    return "economical transportation choices suggest"
                elif cost < 1000:
                    return "standard transportation budgets work well for"
                else:
                    return "premium transportation options are ideal for"
        
        # Nationality context
        if 'nationality' in feature_lower:
            return f"travelers from your background commonly visit"
        
        # Gender context
        if 'gender' in feature_lower:
            return "travelers with similar profiles often choose"
        
        # Accommodation type context
        if 'accommodation' in feature_lower and 'type' in feature_lower:
            return f"your preference for this accommodation style matches well with"
        
        # Transportation type context
        if 'transportation' in feature_lower and 'type' in feature_lower:
            return f"your choice of transportation is commonly used for reaching"
        
        return "this characteristic is associated with"

    def _get_destination_insights(self, destination_name: str) -> str:
        """Provide insights about why someone might want to visit this destination."""
        dest_lower = destination_name.lower()
        
        insights = {
            'amsterdam': "its charming canals, world-class museums, and vibrant cultural scene",
            'paris': "its romantic atmosphere, iconic landmarks, and exceptional cuisine", 
            'tokyo': "its unique blend of traditional culture and cutting-edge technology",
            'new york': "its fast-paced energy, Broadway shows, and diverse neighborhoods",
            'london': "its rich history, royal heritage, and thriving arts scene",
            'bali': "its tropical paradise setting, spiritual culture, and beautiful beaches",
            'bangkok': "its bustling street life, incredible food scene, and ornate temples",
            'sydney': "its stunning harbor, outdoor lifestyle, and friendly atmosphere",
            'rome': "its ancient history, incredible architecture, and authentic Italian culture",
            'barcelona': "its unique architecture, Mediterranean lifestyle, and artistic heritage",
            'berlin': "its dynamic cultural scene, historical significance, and vibrant nightlife",
            'dubai': "its luxurious shopping, modern architecture, and desert adventures",
            'marrakech': "its exotic souks, stunning architecture, and rich cultural traditions"
        }
        
        for key, description in insights.items():
            if key in dest_lower:
                return description
        
        return "its unique cultural attractions and travel experiences"

    def _generate_confidence_explanation(self, probability: float, contributions: list) -> str:
        """Explain why the confidence level is what it is."""
        if probability > 0.7:
            return "The high confidence comes from strong alignment between your travel preferences and this destination's characteristics."
        elif probability > 0.4:
            return "The moderate confidence indicates a good match, though some factors suggest other destinations might also appeal to you."
        elif probability > 0.2:
            return "The lower confidence suggests this destination could work for you, but there are mixed signals in your preferences."
        else:
            return "The low confidence indicates this destination may not be the best match based on your stated preferences."

    def _generate_deep_narrative(self, destination_name: str, shap_data: Dict[str, Any]) -> str:
        """Generate comprehensive, detailed explanation narrative with real SHAP-based weights."""
        if not shap_data or "contributions" not in shap_data or not shap_data["contributions"]:
            return self._generate_fallback_explanation(destination_name)

        final_prob = shap_data['final_probability']
        contributions = shap_data['contributions']
        
        # Structure the narrative similar to explain.txt
        narrative_parts = [
            f"Predicted Next Travel Destination: {destination_name}",
            f"Confidence: {final_prob:.0%}",
            "Why this prediction was made:",
            self._generate_prediction_summary(destination_name, final_prob, contributions),
            "",
            "Key factors influencing this prediction:"
        ]
        
        # Categorize contributions by type and calculate real weights
        pattern_factors = []
        interest_factors = []
        demographic_factors = []
        other_factors = []
        
        total_abs_contribution = sum(abs(item['contribution']) for item in contributions)
        
        for item in contributions:
            feature_name = item['feature'].lower()
            if any(keyword in feature_name for keyword in ['duration', 'cost', 'budget', 'accommodation', 'transportation']):
                pattern_factors.append(item)
            elif any(keyword in feature_name for keyword in ['preference', 'interest', 'activity']):
                interest_factors.append(item)
            elif any(keyword in feature_name for keyword in ['age', 'group', 'traveler']):
                demographic_factors.append(item)
            else:
                other_factors.append(item)
        
        # Calculate actual weights based on SHAP contributions
        def calculate_category_weight(factor_list):
            if not factor_list or total_abs_contribution == 0:
                return 0
            category_contribution = sum(abs(item['contribution']) for item in factor_list)
            return (category_contribution / total_abs_contribution) * 100
        
        pattern_weight = calculate_category_weight(pattern_factors)
        interest_weight = calculate_category_weight(interest_factors)
        demographic_weight = calculate_category_weight(demographic_factors)
        other_weight = calculate_category_weight(other_factors)
        
        # Add sections with real calculated weights
        if pattern_factors:
            narrative_parts.extend([
                f"Travel Pattern Analysis ({pattern_weight:.1f}% importance):",
                ""
            ])
            
            for item in pattern_factors:
                explanation = self._generate_detailed_factor_explanation(item, destination_name)
                narrative_parts.append(explanation)
            
            narrative_parts.append("")
        
        if interest_factors:
            narrative_parts.extend([
                f"Interest Indicators ({interest_weight:.1f}% importance):",
                ""
            ])
            
            for item in interest_factors:
                explanation = self._generate_detailed_factor_explanation(item, destination_name)
                narrative_parts.append(explanation)
            
            narrative_parts.append("")
        
        if demographic_factors:
            narrative_parts.extend([
                f"Demographic Factors ({demographic_weight:.1f}% importance):",
                ""
            ])
            
            for item in demographic_factors:
                explanation = self._generate_detailed_factor_explanation(item, destination_name)
                narrative_parts.append(explanation)
            
            narrative_parts.append("")
        
        if other_factors:
            narrative_parts.extend([
                f"Other Factors ({other_weight:.1f}% importance):",
                ""
            ])
            
            for item in other_factors:
                explanation = self._generate_detailed_factor_explanation(item, destination_name)
                narrative_parts.append(explanation)
            
            narrative_parts.append("")
        
        # Add SHAP methodology explanation
        narrative_parts.extend([
            "How this prediction was made:",
            "",
            "SHAP (SHapley Additive exPlanations) Analysis:",
            f"‚Ä¢ Our AI model analyzed {len(contributions)} key features from your travel profile",
            f"‚Ä¢ Each feature was assigned an importance score based on how much it influenced the {destination_name} prediction",
            f"‚Ä¢ The percentages above represent the actual contribution strength of each category to this specific prediction",
            f"‚Ä¢ SHAP values explain exactly why the model chose {destination_name} over {len(self.target_encoders.get('destination_prediction', {}).classes_) - 1} other destinations",
            "",
            "Model Decision Process:",
            f"‚Ä¢ Base probability: {shap_data.get('base_value', 0):.1%} (random chance across all destinations)",
            f"‚Ä¢ Your profile adjustments: {final_prob - shap_data.get('base_value', final_prob):.1%}",
            f"‚Ä¢ Final confidence: {final_prob:.1%}",
            "",
            f"This explanation is based on real-time analysis of your specific inputs compared to patterns from {self.dataset_stats.get('overall', {}).get('total_trips', 'thousands of')} historical trips."
        ])
        
        return "\n".join(narrative_parts)

    def _generate_prediction_summary(self, destination_name: str, probability: float, contributions: List[Dict]) -> str:
        """Generate a summary explaining why this destination was predicted using real data."""
        positive_factors = [item for item in contributions if item['contribution'] > 0]
        dest_stats = self.dataset_stats.get('destinations', {}).get(destination_name, {})
        overall_stats = self.dataset_stats.get('overall', {})
        
        # Create a contextual summary based on the destination and factors
        summary_parts = []
        
        # Add destination popularity context
        destination_popularity = overall_stats.get('destination_popularity', {})
        if destination_popularity and destination_name in destination_popularity:
            total_trips = sum(destination_popularity.values())
            dest_trips = destination_popularity[destination_name]
            popularity_percentage = (dest_trips / total_trips) * 100
            
            if probability > 0.7:
                summary_parts.append(f"Your travel profile shows a strong alignment with {destination_name} visitors.")
                if popularity_percentage > 10:
                    summary_parts.append(f"{destination_name} is a popular destination in our dataset ({popularity_percentage:.1f}% of all trips), with patterns that closely match your preferences.")
            elif probability > 0.4:
                summary_parts.append(f"Your preferences indicate {destination_name} could be a good match for your next trip.")
                if dest_stats.get('total_trips', 0) > 0:
                    summary_parts.append(f"Based on analysis of {dest_stats['total_trips']} similar trips to {destination_name}, several factors align with your profile.")
            else:
                summary_parts.append(f"While {destination_name} wasn't the top match, several factors suggest it could still appeal to you.")
                if dest_stats.get('total_trips', 0) > 0:
                    summary_parts.append(f"Our analysis of {dest_stats['total_trips']} trips to {destination_name} shows some alignment with your travel style.")
        else:
            # Fallback to original logic
            if probability > 0.7:
                summary_parts.append(f"Your travel profile shows a strong alignment with {destination_name} visitors.")
            elif probability > 0.4:
                summary_parts.append(f"Your preferences indicate {destination_name} could be a good match for your next trip.")
            else:
                summary_parts.append(f"While {destination_name} wasn't the top match, several factors suggest it could still appeal to you.")
        
        # Add specific insights based on top contributing factors with real data
        if positive_factors:
            top_factor = max(positive_factors, key=lambda x: abs(x['contribution']))
            factor_insight = self._get_factor_insight_with_data(top_factor, destination_name, dest_stats)
            if factor_insight:
                summary_parts.append(factor_insight)
        
        return " ".join(summary_parts)

    def _get_factor_insight_with_data(self, factor_item: Dict, destination_name: str, dest_stats: Dict) -> str:
        """Get insight about the most influential factor using real dataset statistics."""
        feature_name = factor_item['feature'].lower()
        user_value = factor_item['user_value']
        
        if 'duration' in feature_name and dest_stats:
            avg_duration = dest_stats.get('avg_duration', 0)
            if avg_duration > 0:
                return f"Your trip length particularly aligns with the {destination_name} average of {avg_duration:.1f} days."
            else:
                return f"Your preferred trip length aligns well with typical {destination_name} itineraries."
        elif ('budget' in feature_name or 'cost' in feature_name) and dest_stats:
            avg_total = dest_stats.get('avg_total_cost', 0)
            if avg_total > 0:
                return f"Your budget range matches the typical {destination_name} trip cost of ${avg_total:.0f}."
            else:
                return f"Your budget range matches the typical spending patterns of {destination_name} visitors."
        elif 'age' in feature_name and dest_stats:
            age_demo = dest_stats.get('age_demographics', {})
            avg_age = age_demo.get('avg_age', 0)
            if avg_age > 0:
                return f"Your age demographic aligns with the typical {destination_name} traveler (average age: {avg_age:.0f})."
            else:
                return f"Travelers in your demographic frequently choose {destination_name} as their next destination."
        else:
            return f"Your travel preferences show strong indicators for {destination_name} based on historical patterns."

    def _generate_detailed_factor_explanation(self, factor_item: Dict, destination_name: str) -> str:
        """Generate detailed explanation for a specific factor similar to explain.txt style."""
        feature_name = factor_item['feature'].replace('_', ' ').title()
        user_value = factor_item['user_value']
        contribution = factor_item['contribution']
        
        # Generate contextual explanation based on feature type and value
        if contribution > 0:
            explanation = self._get_positive_factor_explanation(feature_name, user_value, destination_name)
        else:
            explanation = self._get_negative_factor_explanation(feature_name, user_value, destination_name)
        
        return explanation

    def _get_factor_insight(self, factor_item: Dict, destination_name: str) -> str:
        """Get insight about the most influential factor."""
        feature_name = factor_item['feature'].lower()
        user_value = factor_item['user_value']
        
        if 'duration' in feature_name:
            return f"Your preferred trip length aligns well with typical {destination_name} itineraries."
        elif 'budget' in feature_name or 'cost' in feature_name:
            return f"Your budget range matches the typical spending patterns of {destination_name} visitors."
        elif 'age' in feature_name:
            return f"Travelers in your demographic frequently choose {destination_name} as their next destination."
        else:
            return f"Your travel preferences show strong indicators for {destination_name}."

    def _get_positive_factor_explanation(self, feature_name: str, user_value: Any, destination_name: str) -> str:
        """Generate positive factor explanation using actual dataset statistics."""
        feature_lower = feature_name.lower()
        
        # Get destination-specific stats if available
        dest_stats = self.dataset_stats.get('destinations', {}).get(destination_name, {})
        overall_stats = self.dataset_stats.get('overall', {})
        
        if 'duration' in feature_lower and dest_stats:
            avg_duration = dest_stats.get('avg_duration', 0)
            if avg_duration > 0:
                try:
                    user_days = float(str(user_value).replace(' days', ''))
                    if abs(user_days - avg_duration) <= 2:
                        return f"Your trip duration of {user_value} days matches the average {destination_name} trip length ({avg_duration:.1f} days)"
                    else:
                        return f"Your trip duration of {user_value} days is within the typical range for {destination_name} (average: {avg_duration:.1f} days)"
                except:
                    return f"Your trip duration of {user_value} days aligns with typical {destination_name} itineraries"
            else:
                return f"Your trip duration of {user_value} days aligns with typical {destination_name} itineraries"
        
        elif ('budget' in feature_lower or 'cost' in feature_lower) and dest_stats:
            if 'accommodation' in feature_lower:
                avg_cost = dest_stats.get('avg_accommodation_cost', 0)
                cost_range = dest_stats.get('accommodation_cost_range', (0, 0))
                if avg_cost > 0:
                    try:
                        user_cost = float(str(user_value).replace('$', '').replace(',', ''))
                        percentage_diff = abs(user_cost - avg_cost) / avg_cost * 100
                        if percentage_diff <= 20:
                            return f"Your accommodation budget ({user_value}) closely matches the average for {destination_name} travelers (${avg_cost:.0f})"
                        else:
                            return f"Your accommodation budget ({user_value}) is within the typical range for {destination_name} (${cost_range[0]:.0f}-${cost_range[1]:.0f})"
                    except:
                        return f"Your accommodation budget ({user_value}) fits well within typical {destination_name} spending patterns"
            elif 'transportation' in feature_lower:
                avg_cost = dest_stats.get('avg_transportation_cost', 0)
                if avg_cost > 0:
                    try:
                        user_cost = float(str(user_value).replace('$', '').replace(',', ''))
                        percentage_diff = abs(user_cost - avg_cost) / avg_cost * 100
                        if percentage_diff <= 20:
                            return f"Your transportation budget ({user_value}) aligns with the average for {destination_name} (${avg_cost:.0f})"
                        else:
                            return f"Your transportation budget ({user_value}) is within the typical range for {destination_name} travelers"
                    except:
                        return f"Your transportation budget ({user_value}) aligns with typical {destination_name} travel costs"
            else:
                avg_total = dest_stats.get('avg_total_cost', 0)
                if avg_total > 0:
                    return f"Your total budget fits well within the average {destination_name} trip cost (${avg_total:.0f})"
                else:
                    return f"Your budget fits well within typical {destination_name} spending patterns"
        
        elif 'age' in feature_lower and dest_stats:
            age_demo = dest_stats.get('age_demographics', {})
            avg_age = age_demo.get('avg_age', 0)
            age_dist = age_demo.get('age_distribution', {})
            if avg_age > 0:
                try:
                    user_age = float(str(user_value).replace(' years old', '').replace(' years', ''))
                    age_diff = abs(user_age - avg_age)
                    if age_diff <= 5:
                        return f"Your age ({user_value}) matches the typical {destination_name} traveler (average age: {avg_age:.0f})"
                    else:
                        # Find user's age group
                        user_age_group = None
                        if user_age <= 25:
                            user_age_group = '18-25'
                        elif user_age <= 35:
                            user_age_group = '26-35'
                        elif user_age <= 50:
                            user_age_group = '36-50'
                        elif user_age <= 65:
                            user_age_group = '51-65'
                        else:
                            user_age_group = '65+'
                        
                        group_count = age_dist.get(user_age_group, 0)
                        total_travelers = sum(age_dist.values()) if age_dist else 1
                        if group_count > 0 and total_travelers > 0:
                            percentage = (group_count / total_travelers) * 100
                            return f"Your age group ({user_age_group}) represents {percentage:.0f}% of {destination_name} travelers in our data"
                        else:
                            return f"Travelers in your age group ({user_value}) frequently choose {destination_name}"
                except:
                    return f"Your age ({user_value}) aligns with typical {destination_name} visitor demographics"
            else:
                return f"Travelers in your age group ({user_value}) frequently choose {destination_name}"
        
        elif 'accommodation' in feature_lower and dest_stats:
            popular_types = dest_stats.get('popular_accommodation_types', {})
            if popular_types and str(user_value) in popular_types:
                total_bookings = sum(popular_types.values())
                user_type_count = popular_types[str(user_value)]
                percentage = (user_type_count / total_bookings) * 100
                return f"Your accommodation choice ({user_value}) is popular in {destination_name} ({percentage:.0f}% of travelers choose this type)"
            else:
                return f"Your accommodation preference ({user_value}) matches available options in {destination_name}"
        
        else:
            return f"Your {feature_name.lower()} ({user_value}) is a strong indicator for {destination_name} based on historical patterns"

    def _get_negative_factor_explanation(self, feature_name: str, user_value: Any, destination_name: str) -> str:
        """Generate negative factor explanation with constructive feedback using real data."""
        feature_lower = feature_name.lower()
        
        # Get destination-specific stats if available
        dest_stats = self.dataset_stats.get('destinations', {}).get(destination_name, {})
        
        if 'duration' in feature_lower and dest_stats:
            avg_duration = dest_stats.get('avg_duration', 0)
            duration_range = dest_stats.get('duration_range', (0, 0))
            if avg_duration > 0:
                try:
                    user_days = float(str(user_value).replace(' days', ''))
                    if user_days < avg_duration:
                        return f"Your trip length ({user_value} days) is shorter than typical {destination_name} visits (average: {avg_duration:.1f} days), but shorter trips can still be rewarding"
                    else:
                        return f"Your trip length ({user_value} days) is longer than typical {destination_name} visits (average: {avg_duration:.1f} days), allowing for more in-depth exploration"
                except:
                    return f"Your preferred trip length ({user_value}) differs from typical {destination_name} visits, but can still work well"
            else:
                return f"Your preferred trip length ({user_value}) differs from typical {destination_name} visits, but can still work well"
        
        elif ('budget' in feature_lower or 'cost' in feature_lower) and dest_stats:
            if 'accommodation' in feature_lower:
                avg_cost = dest_stats.get('avg_accommodation_cost', 0)
                cost_range = dest_stats.get('accommodation_cost_range', (0, 0))
                if avg_cost > 0:
                    try:
                        user_cost = float(str(user_value).replace('$', '').replace(',', ''))
                        if user_cost < avg_cost:
                            return f"Your accommodation budget ({user_value}) is below the {destination_name} average (${avg_cost:.0f}), but budget-friendly options are available"
                        else:
                            return f"Your accommodation budget ({user_value}) exceeds the {destination_name} average (${avg_cost:.0f}), offering premium options"
                    except:
                        return f"Your accommodation budget ({user_value}) differs from typical {destination_name} spending, but options exist in your range"
            elif 'transportation' in feature_lower:
                avg_cost = dest_stats.get('avg_transportation_cost', 0)
                if avg_cost > 0:
                    try:
                        user_cost = float(str(user_value).replace('$', '').replace(',', ''))
                        if user_cost < avg_cost:
                            return f"Your transportation budget ({user_value}) is below the {destination_name} average (${avg_cost:.0f}), but economical options exist"
                        else:
                            return f"Your transportation budget ({user_value}) exceeds typical {destination_name} costs (${avg_cost:.0f}), allowing for premium travel options"
                    except:
                        return f"Your transportation budget ({user_value}) differs from typical {destination_name} costs, but alternatives are available"
            else:
                avg_total = dest_stats.get('avg_total_cost', 0)
                if avg_total > 0:
                    return f"Your budget differs from the average {destination_name} trip cost (${avg_total:.0f}), but travel options exist for various budgets"
                else:
                    return f"Your budget differs from typical {destination_name} spending, but options exist in your range"
        
        elif 'age' in feature_lower and dest_stats:
            age_demo = dest_stats.get('age_demographics', {})
            avg_age = age_demo.get('avg_age', 0)
            age_dist = age_demo.get('age_distribution', {})
            if avg_age > 0:
                try:
                    user_age = float(str(user_value).replace(' years old', '').replace(' years', ''))
                    # Find user's age group
                    user_age_group = None
                    if user_age <= 25:
                        user_age_group = '18-25'
                    elif user_age <= 35:
                        user_age_group = '26-35'
                    elif user_age <= 50:
                        user_age_group = '36-50'
                    elif user_age <= 65:
                        user_age_group = '51-65'
                    else:
                        user_age_group = '65+'
                    
                    group_count = age_dist.get(user_age_group, 0)
                    total_travelers = sum(age_dist.values()) if age_dist else 1
                    if group_count > 0 and total_travelers > 0:
                        percentage = (group_count / total_travelers) * 100
                        if percentage < 10:
                            return f"Your age group ({user_age_group}) represents a smaller portion of {destination_name} visitors ({percentage:.0f}%), but the destination welcomes all ages"
                        else:
                            return f"Your age group ({user_age_group}) represents {percentage:.0f}% of {destination_name} travelers, showing diverse appeal"
                    else:
                        return f"While {destination_name} attracts all age groups, your demographic ({user_value}) shows slightly different typical preferences"
                except:
                    return f"Your age ({user_value}) differs from the typical {destination_name} visitor profile, but the destination suits various demographics"
            else:
                return f"While {destination_name} attracts all age groups, your demographic ({user_value}) shows slightly different typical preferences"
        
        else:
            return f"Your {feature_name.lower()} ({user_value}) doesn't strongly correlate with typical {destination_name} visitors, but individual preferences vary and the destination offers diverse experiences"

    def _generate_fallback_explanation(self, destination_name: str) -> str:
        """Generate explanation when SHAP data is unavailable."""
        destination_insights = self._get_destination_insights(destination_name)
        
        return f"""üéØ **{destination_name}** appears in your recommendations based on general travel patterns.

{destination_name} is known for {destination_insights}.

**Why you're seeing this recommendation:**
Our model identified {destination_name} as a potential match based on your travel preferences, though we weren't able to generate a detailed breakdown of the specific factors. This sometimes happens with less common preference combinations.

**What this means:**
‚Ä¢ {destination_name} has been popular with travelers who have similar basic characteristics to you
‚Ä¢ Consider researching this destination to see if its attractions align with your interests
‚Ä¢ The recommendation is based on successful trips by travelers with comparable profiles

**üí° Tip:** If you're curious about this destination, try adjusting some of your preferences (like trip duration or budget) to see how it affects your match score."""

    def _get_improvement_suggestion(self, feature_name: str, value: Any, destination_name: str) -> str:
        """Provide constructive suggestions for improving destination match."""
        feature_lower = feature_name.lower()
        
        if 'duration' in feature_lower:
            days = float(value) if isinstance(value, (int, float)) else 7
            if days <= 3:
                return f"Short trips might not allow you to fully experience {destination_name}. Consider extending to 5-7 days if possible."
            elif days > 14:
                return f"Very long stays might be more budget-intensive. {destination_name} can be wonderful in 7-10 days."
        
        elif 'cost' in feature_lower:
            if 'accommodation' in feature_lower:
                return f"Your accommodation budget might limit options in {destination_name}. Consider flexible dates or alternative neighborhoods."
            elif 'transportation' in feature_lower:
                return f"Transportation costs to {destination_name} vary by season and booking timing. Booking in advance often helps."
        
        elif 'age' in feature_lower:
            return f"While {destination_name} welcomes all ages, some activities might be more popular with different age groups."
        
        return f"This factor shows some mismatch, but {destination_name} offers diverse experiences that might still appeal to you."

    def _generate_improvement_tips(self, destination_name: str, negative_factors: list) -> str:
        """Generate actionable tips for improving destination match."""
        tips = []
        
        for factor in negative_factors[:2]:  # Focus on top 2 negative factors
            feature_name = factor['feature'].lower()
            
            if 'duration' in feature_name:
                tips.append("Consider adjusting your trip length to 7-10 days for the full experience")
            elif 'cost' in feature_name:
                tips.append("Look into shoulder season travel or alternative accommodations to optimize your budget")
            elif 'accommodation' in feature_name:
                tips.append("Explore different accommodation types that might better suit the local culture")
            elif 'transportation' in feature_name:
                tips.append("Consider alternative transportation options that might enhance your journey")
        
        if not tips:
            tips.append(f"Research what makes {destination_name} special to see if it aligns with your interests")
        
        return "‚Ä¢ " + "\n‚Ä¢ ".join(tips)

    def analyze_model_bias(self, task_name: str = 'destination_prediction', 
                      output_dir: str = 'bias_analysis') -> dict:
        """
        CORRECTED: Comprehensive bias analysis that actually generates and saves plots.
        """
        if not self.is_trained or self.X_test_full is None:
            print("‚ùå Error: Model must be trained first with full test data")
            return {}
        
        print("üîç Starting FIXED bias analysis...")
        
        # Create output directory with proper error handling
        try:
            os.makedirs(output_dir, exist_ok=True)
            print(f"‚úÖ Output directory created: {output_dir}")
        except Exception as e:
            print(f"‚ùå Could not create output directory: {e}")
            return {}
        
        bias_results = {}
        
        try:
            # Get model and predictions
            model = self.models[task_name]['model']
            X_test = self.X_test_full
            y_test = self.y_test_full
            
            # Generate predictions
            print("üîÑ Generating predictions for bias analysis...")
            y_pred = model.predict(X_test)
            y_proba = model.predict_proba(X_test)
            print(f"‚úÖ Generated predictions for {len(y_pred)} samples")
            
            # Create synthetic test data for demonstration (since we might not have original)
            test_data_original = self._create_synthetic_test_data_for_bias(len(X_test))
            
            if test_data_original is not None:
                print(f"‚úÖ Test data prepared: {len(test_data_original)} samples")
                
                # 1. Demographic Bias Analysis
                print("üìä Analyzing demographic bias...")
                bias_results['demographic'] = self._analyze_demographic_bias(
                    test_data_original, y_pred, y_proba, output_dir
                )
                
                # 2. Economic Bias Analysis  
                print("üí∞ Analyzing economic bias...")
                bias_results['economic'] = self._analyze_economic_bias(
                    test_data_original, y_pred, y_proba, output_dir
                )
                
                # 3. Geographic Bias Analysis
                print("üó∫Ô∏è Analyzing geographic bias...")
                bias_results['geographic'] = self._analyze_geographic_bias(
                    test_data_original, y_pred, y_proba, output_dir
                )
                
                # 4. Generate summary report
                self._generate_bias_report(bias_results, output_dir)
                
                print(f"‚úÖ Bias analysis completed! Results saved to: {output_dir}")
                
                # List generated files
                self._list_generated_bias_files(output_dir)
                
            else:
                print("‚ùå Could not prepare test data for bias analysis")
                
        except Exception as e:
            print(f"‚ùå Bias analysis failed: {e}")
            import traceback
            traceback.print_exc()
        
        return bias_results

    def _create_synthetic_test_data_for_bias(self, n_samples: int) -> pd.DataFrame:
        """Create synthetic test data for bias analysis when original data isn't available."""
        try:
            # Try to use the existing method first
            original_data = self._reconstruct_original_test_data()
            if original_data is not None and len(original_data) >= n_samples:
                return original_data.head(n_samples)
            
            # If that fails, create synthetic data with the required columns
            print("Creating synthetic test data for bias analysis...")
            
            # Demographics
            genders = ['Male', 'Female', 'Other']
            ages = np.random.randint(18, 70, n_samples)
            nationalities = ['USA', 'UK', 'Canada', 'Australia', 'Germany', 'France', 'Japan', 'Brazil', 'India', 'Mexico']
            
            # Trip characteristics
            durations = np.random.randint(1, 30, n_samples)
            accommodations = ['Hotel', 'Hostel', 'Airbnb', 'Resort', 'Apartment']
            purposes = ['Leisure', 'Business', 'Education', 'Medical', 'Family Visit']
            
            # Create DataFrame
            synthetic_data = pd.DataFrame({
                'Traveler gender': np.random.choice(genders, n_samples),
                'Traveler age': ages,
                'Traveler nationality': np.random.choice(nationalities, n_samples),
                'Trip duration': durations,
                'Accommodation type': np.random.choice(accommodations, n_samples),
                'Trip purpose': np.random.choice(purposes, n_samples),
                'Total cost': np.random.uniform(500, 5000, n_samples)
            })
            
            print(f"‚úÖ Created synthetic test data with {len(synthetic_data)} samples")
            return synthetic_data
            
        except Exception as e:
            print(f"‚ùå Failed to create synthetic test data: {e}")
            return None

    def _reconstruct_original_test_data(self) -> pd.DataFrame:
        """Reconstruct original test data for bias analysis."""
        # For this implementation, we'll use the current test data
        # In a production system, you'd want to store the original test data
        try:
            # Load the original dataset and reconstruct test portion
            raw_data = pd.read_csv('extended_travel_trips.csv')
            raw_data['Start date'] = pd.to_datetime(raw_data['Start date'], errors='coerce')
            raw_data = raw_data.sort_values(by='Start date').reset_index(drop=True)
            
            # Use same split as in training (80/20)
            split_index = int(len(raw_data) * 0.8)
            test_df = raw_data.iloc[split_index:].reset_index(drop=True)
            
            # Ensure we have the right number of samples
            if len(test_df) == len(self.X_test_full):
                return test_df
            else:
                print(f"‚ö†Ô∏è Test data size mismatch: {len(test_df)} vs {len(self.X_test_full)}")
                return test_df.head(len(self.X_test_full))
                
        except Exception as e:
            print(f"Warning: Could not reconstruct test data: {e}")
            return None

    def _analyze_demographic_bias(self, test_data: pd.DataFrame, y_pred: np.ndarray, 
                               y_proba: np.ndarray, output_dir: str) -> dict:
        """
        Analyze demographic bias with proper error handling and file saving.
        """
        print("üìä Analyzing demographic bias...")
        
        results = {}
        class_names = self.target_encoders['destination_prediction'].classes_
        
        try:
            # Gender bias analysis
            if 'Traveler gender' in test_data.columns:
                print("  - Analyzing gender bias...")
                gender_bias = self._calculate_group_bias(
                    test_data, 'Traveler gender', y_pred, y_proba, class_names
                )
                results['gender'] = gender_bias
                self._plot_demographic_bias(gender_bias, 'Gender', output_dir)
            
            # Age bias analysis
            if 'Traveler age' in test_data.columns:
                print("  - Analyzing age bias...")
                # Create age groups
                test_data = test_data.copy()
                test_data['Age_Group'] = pd.cut(test_data['Traveler age'], 
                                            bins=[0, 25, 35, 50, 65, 100], 
                                            labels=['18-25', '26-35', '36-50', '51-65', '65+'])
                age_bias = self._calculate_group_bias(
                    test_data, 'Age_Group', y_pred, y_proba, class_names
                )
                results['age'] = age_bias
                self._plot_demographic_bias(age_bias, 'Age_Group', output_dir)
            
            # Nationality bias analysis
            if 'Traveler nationality' in test_data.columns:
                print("  - Analyzing nationality bias...")
                nationality_bias = self._calculate_group_bias(
                    test_data, 'Traveler nationality', y_pred, y_proba, class_names
                )
                results['nationality'] = nationality_bias
                self._plot_demographic_bias(nationality_bias, 'Nationality', output_dir)
            
            print("‚úÖ Demographic bias analysis completed")
            
        except Exception as e:
            print(f"‚ùå Demographic bias analysis failed: {e}")
        
        return results


    def _analyze_economic_bias(self, test_data: pd.DataFrame, y_pred: np.ndarray, 
                            y_proba: np.ndarray, output_dir: str) -> dict:
        """
        Analyze economic bias with proper plotting.
        """
        print("üí∞ Analyzing economic bias...")
        
        results = {}
        class_names = self.target_encoders['destination_prediction'].classes_
        
        try:
            # Create budget categories
            if 'Accommodation cost' in test_data.columns and 'Transportation cost' in test_data.columns:
                test_data = test_data.copy()
                test_data['Total_Budget'] = test_data['Accommodation cost'] + test_data['Transportation cost']
                test_data['Budget_Category'] = pd.cut(test_data['Total_Budget'], 
                                                    bins=[0, 1000, 2000, 3000, float('inf')], 
                                                    labels=['Budget (<$1k)', 'Mid-range ($1-2k)', 
                                                        'Premium ($2-3k)', 'Luxury (>$3k)'])
                
                budget_bias = self._calculate_group_bias(
                    test_data, 'Budget_Category', y_pred, y_proba, class_names
                )
                results['budget'] = budget_bias
                
                # Plot economic bias
                self._plot_economic_bias(budget_bias, output_dir)
                
                print("‚úÖ Economic bias analysis completed")
            else:
                print("‚ö†Ô∏è Cost columns not available for economic bias analysis")
        
        except Exception as e:
            print(f"‚ùå Economic bias analysis failed: {e}")
        
        return results


    def _analyze_geographic_bias(self, test_data: pd.DataFrame, y_pred: np.ndarray, 
                             y_proba: np.ndarray, output_dir: str) -> dict:
        """
        Analyze geographic bias with proper plotting.
        """
        print("üó∫Ô∏è Analyzing geographic bias...")
        
        results = {}
        class_names = self.target_encoders['destination_prediction'].classes_
        
        try:
            # Create continent mapping
            continent_mapping = {
                'Amsterdam': 'Europe', 'Athens': 'Europe', 'Barcelona': 'Europe', 
                'Berlin': 'Europe', 'Edinburgh': 'Europe', 'London': 'Europe', 
                'Paris': 'Europe', 'Rome': 'Europe',
                'Bangkok': 'Asia', 'Tokyo': 'Asia', 'Seoul': 'Asia', 'Bali': 'Asia',
                'Dubai': 'Asia', 'Phuket': 'Asia',
                'New York': 'North America', 'Los Angeles': 'North America', 'Vancouver': 'North America',
                'Cancun': 'North America', 'Honolulu': 'North America',
                'Sydney': 'Oceania', 'Auckland': 'Oceania',
                'Cape Town': 'Africa', 'Marrakech': 'Africa',
                'Rio de Janeiro': 'South America'
            }
            
            # Map predicted destinations to continents
            pred_destinations = [class_names[i] for i in y_pred]
            pred_continents = []
            
            for dest in pred_destinations:
                dest_city = dest.split(',')[0] if ',' in dest else dest
                continent = continent_mapping.get(dest_city, 'Other')
                pred_continents.append(continent)
            
            # Analyze continent distribution
            continent_dist = pd.Series(pred_continents).value_counts(normalize=True)
            results['continent_distribution'] = continent_dist.to_dict()
            
            # Plot geographic bias
            self._plot_geographic_bias(continent_dist, output_dir)
            
            print("‚úÖ Geographic bias analysis completed")
            
        except Exception as e:
            print(f"‚ùå Geographic bias analysis failed: {e}")
        
        return results

    def _analyze_cultural_bias(self, test_data: pd.DataFrame, y_pred: np.ndarray, 
                             y_proba: np.ndarray, output_dir: str) -> Dict[str, Any]:
        """Analyze cultural biases and stereotypical associations."""
        print("üåç Analyzing cultural bias...")
        
        results = {}
        class_names = self.target_encoders['destination_prediction'].classes_
        
        # Analyze nationality -> destination patterns
        if 'Traveler nationality' in test_data.columns:
            nationality_dest_patterns = {}
            
            for nationality in test_data['Traveler nationality'].unique():
                mask = test_data['Traveler nationality'] == nationality
                if mask.sum() > 10:  # Only analyze if we have enough samples
                    nat_predictions = y_pred[mask]
                    nat_destinations = [class_names[i] for i in nat_predictions]
                    dest_counts = pd.Series(nat_destinations).value_counts(normalize=True).head(5)
                    nationality_dest_patterns[nationality] = dest_counts.to_dict()
            
            results['nationality_patterns'] = nationality_dest_patterns
            self._plot_cultural_bias(nationality_dest_patterns, output_dir)
        
        return results

    def _calculate_group_bias(self, data: pd.DataFrame, group_col: str, y_pred: np.ndarray, 
                          y_proba: np.ndarray, class_names: np.ndarray) -> dict:
        """
        Calculate bias metrics with proper error handling.
        """
        bias_metrics = {}
        
        try:
            for group in data[group_col].unique():
                if pd.isna(group):
                    continue
                    
                mask = data[group_col] == group
                group_size = mask.sum()
                
                if group_size < 5:  # Skip groups with too few samples
                    continue
                
                group_pred = y_pred[mask]
                group_proba = y_proba[mask]
                
                # Calculate metrics safely
                pred_distribution = pd.Series([class_names[i] for i in group_pred]).value_counts(normalize=True)
                avg_confidence = np.mean(np.max(group_proba, axis=1))
                
                bias_metrics[str(group)] = {
                    'sample_count': int(group_size),
                    'top_destinations': pred_distribution.head(3).to_dict(),
                    'average_confidence': float(avg_confidence),
                    'destination_diversity': len(pred_distribution)
                }
                
            print(f"  ‚úÖ Calculated bias metrics for {len(bias_metrics)} groups in {group_col}")
            
        except Exception as e:
            print(f"  ‚ùå Error calculating bias for {group_col}: {e}")
        
        return bias_metrics


    def _plot_demographic_bias(self, bias_data: dict, bias_type: str, output_dir: str):
        """
        Plot demographic bias with guaranteed file saving.
        """
        if not bias_data:
            print(f"  ‚ö†Ô∏è No bias data to plot for {bias_type}")
            return
        
        try:
            print(f"  üé® Creating {bias_type} bias plot...")
            
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            
            groups = list(bias_data.keys())
            sample_counts = [bias_data[g]['sample_count'] for g in groups]
            avg_confidences = [bias_data[g]['average_confidence'] for g in groups]
            diversities = [bias_data[g]['destination_diversity'] for g in groups]
            
            # Sample distribution
            bars1 = ax1.bar(groups, sample_counts, color='skyblue', alpha=0.7)
            ax1.set_title(f'{bias_type} - Sample Distribution', fontsize=14, fontweight='bold')
            ax1.set_ylabel('Number of Samples')
            ax1.tick_params(axis='x', rotation=45)
            
            # Add value labels
            for bar, count in zip(bars1, sample_counts):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{count}', ha='center', va='bottom')
            
            # Average confidence
            bars2 = ax2.bar(groups, avg_confidences, color='lightcoral', alpha=0.7)
            ax2.set_title(f'{bias_type} - Average Model Confidence', fontsize=14, fontweight='bold')
            ax2.set_ylabel('Average Confidence')
            ax2.tick_params(axis='x', rotation=45)
            ax2.set_ylim(0, 1)
            
            # Add value labels
            for bar, conf in zip(bars2, avg_confidences):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{conf:.3f}', ha='center', va='bottom')
            
            # Destination diversity
            bars3 = ax3.bar(groups, diversities, color='lightgreen', alpha=0.7)
            ax3.set_title(f'{bias_type} - Destination Diversity', fontsize=14, fontweight='bold')
            ax3.set_ylabel('Number of Unique Destinations')
            ax3.tick_params(axis='x', rotation=45)
            
            # Add value labels
            for bar, div in zip(bars3, diversities):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{div}', ha='center', va='bottom')
            
            # Top destinations heatmap
            dest_matrix = []
            all_destinations = set()
            for group in groups:
                top_dests = bias_data[group]['top_destinations']
                all_destinations.update(top_dests.keys())
            
            all_destinations = sorted(list(all_destinations))[:8]  # Top 8 destinations
            
            for group in groups:
                row = [bias_data[group]['top_destinations'].get(dest, 0) for dest in all_destinations]
                dest_matrix.append(row)
            
            if dest_matrix and all_destinations:
                im = ax4.imshow(dest_matrix, cmap='Blues', aspect='auto')
                ax4.set_xticks(range(len(all_destinations)))
                ax4.set_xticklabels([d.split(',')[0] for d in all_destinations], rotation=45, ha='right')
                ax4.set_yticks(range(len(groups)))
                ax4.set_yticklabels(groups)
                ax4.set_title(f'{bias_type} - Top Destination Preferences', fontsize=14, fontweight='bold')
                
                plt.colorbar(im, ax=ax4, label='Prediction Probability')
            else:
                ax4.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax4.transAxes)
                ax4.set_title(f'{bias_type} - No Data Available')
            
            plt.tight_layout()
            
            # Save the plot with guaranteed path
            save_path = os.path.join(output_dir, f"{bias_type.lower()}_bias_analysis.png")
            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
            
            # Verify file was saved
            if os.path.exists(save_path):
                print(f"  ‚úÖ {bias_type} bias plot saved: {save_path}")
            else:
                print(f"  ‚ùå Failed to save {bias_type} bias plot")
            
            plt.show()
            plt.close()  # Close to free memory
            
        except Exception as e:
            print(f"  ‚ùå Error plotting {bias_type} bias: {e}")
            import traceback
            traceback.print_exc()

    def _plot_economic_bias(self, bias_data: Dict[str, Any], output_dir: str):
        """Plot economic bias analysis."""
        if not bias_data:
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        groups = list(bias_data.keys())
        sample_counts = [bias_data[g]['sample_count'] for g in groups]
        avg_confidences = [bias_data[g]['average_confidence'] for g in groups]
        
        # Budget distribution
        ax1.pie(sample_counts, labels=groups, autopct='%1.1f%%', startangle=90)
        ax1.set_title('Sample Distribution by Budget Category', fontsize=14, fontweight='bold')
        
        # Confidence by budget
        ax2.bar(groups, avg_confidences, color=['green', 'blue', 'orange', 'red'], alpha=0.7)
        ax2.set_title('Model Confidence by Budget Category', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Average Confidence')
        ax2.tick_params(axis='x', rotation=45)
        ax2.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/economic_bias_analysis.png", dpi=300, bbox_inches='tight')
        plt.show()

    def _plot_geographic_bias(self, continent_dist: pd.Series, output_dir: str):
        """
        Plot geographic bias with guaranteed saving.
        """
        try:
            print("  üé® Creating geographic bias plot...")
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            # Continent distribution pie chart
            ax1.pie(continent_dist.values, labels=continent_dist.index, autopct='%1.1f%%', startangle=90)
            ax1.set_title('Predicted Destinations by Continent', fontsize=14, fontweight='bold')
            
            # Continent distribution bar chart
            colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown'][:len(continent_dist)]
            bars = ax2.bar(continent_dist.index, continent_dist.values, color=colors, alpha=0.7)
            ax2.set_title('Geographic Distribution of Recommendations', fontsize=14, fontweight='bold')
            ax2.set_ylabel('Proportion of Recommendations')
            ax2.tick_params(axis='x', rotation=45)
            
            # Add value labels
            for bar, val in zip(bars, continent_dist.values):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{val:.3f}', ha='center', va='bottom')
            
            plt.tight_layout()
            
            # Save the plot
            save_path = os.path.join(output_dir, "geographic_bias_analysis.png")
            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
            
            if os.path.exists(save_path):
                print(f"  ‚úÖ Geographic bias plot saved: {save_path}")
            else:
                print(f"  ‚ùå Failed to save geographic bias plot")
            
            plt.show()
            plt.close()
            
        except Exception as e:
            print(f"  ‚ùå Error plotting geographic bias: {e}")

    def _plot_cultural_bias(self, nationality_patterns: Dict[str, Dict], output_dir: str):
        """Plot cultural bias analysis."""
        if not nationality_patterns:
            return
        
        # Create heatmap of nationality -> destination preferences
        nationalities = list(nationality_patterns.keys())[:10]  # Top 10 nationalities
        all_destinations = set()
        for patterns in nationality_patterns.values():
            all_destinations.update(patterns.keys())
        all_destinations = sorted(list(all_destinations))[:15]  # Top 15 destinations
        
        matrix = []
        for nat in nationalities:
            row = [nationality_patterns[nat].get(dest, 0) for dest in all_destinations]
            matrix.append(row)
        
        plt.figure(figsize=(16, 8))
        im = plt.imshow(matrix, cmap='Reds', aspect='auto')
        plt.xticks(range(len(all_destinations)), [d.split(',')[0] for d in all_destinations], 
                  rotation=45, ha='right')
        plt.yticks(range(len(nationalities)), nationalities)
        plt.title('Cultural Bias: Nationality ‚Üí Destination Preferences', fontsize=16, fontweight='bold')
        plt.xlabel('Recommended Destinations')
        plt.ylabel('Traveler Nationality')
        plt.colorbar(im, label='Recommendation Probability')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/cultural_bias_analysis.png", dpi=300, bbox_inches='tight')
        plt.show()

    def _generate_bias_report(self, bias_results: dict, output_dir: str):
        """
        Generate bias report with guaranteed saving.
        """
        try:
            print("üìÑ Generating bias analysis report...")
            
            report_lines = [
                "# AI Model Bias Analysis Report",
                f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                "",
                "## Executive Summary",
                "This report analyzes potential biases in the travel destination recommendation model.",
                "The analysis covers demographic, economic, and geographic dimensions.",
                "",
                "## Analysis Results",
                ""
            ]
            
            # Add findings for each bias type
            for bias_type, results in bias_results.items():
                if results:
                    report_lines.extend([
                        f"### {bias_type.title()} Bias Analysis",
                        f"- Analyzed {len(results)} groups",
                        f"- Generated visualization: {bias_type}_bias_analysis.png",
                        ""
                    ])
            
            # Add file list
            report_lines.extend([
                "## Generated Files",
                "The following files were created in this analysis:",
                ""
            ])
            
            for bias_type in bias_results.keys():
                if bias_results[bias_type]:
                    report_lines.append(f"- {bias_type}_bias_analysis.png")
            
            # Save report
            report_path = os.path.join(output_dir, "bias_analysis_report.md")
            with open(report_path, 'w') as f:
                f.write('\n'.join(report_lines))
            
            if os.path.exists(report_path):
                print(f"  ‚úÖ Bias analysis report saved: {report_path}")
            else:
                print(f"  ‚ùå Failed to save bias analysis report")
            
        except Exception as e:
            print(f"‚ùå Error generating bias report: {e}")

    def _list_generated_bias_files(self, output_dir: str):
        """List all files generated during bias analysis."""
        try:
            files = os.listdir(output_dir)
            if files:
                print(f"\nüìÅ Files generated in {output_dir}:")
                for file in sorted(files):
                    file_path = os.path.join(output_dir, file)
                    file_size = os.path.getsize(file_path)
                    print(f"  ‚úÖ {file} ({file_size} bytes)")
            else:
                print(f"‚ö†Ô∏è No files found in {output_dir}")
        except Exception as e:
            print(f"‚ùå Error listing files: {e}")

    def predict_destination(self, user_data: Union[Dict[str, Any], pd.DataFrame], 
                          task_name: str = 'destination_prediction', 
                          top_k: int = 5) -> Dict[str, Any]:
        """
        Make predictions for user input with detailed explanations.
        
        Args:
            user_data: User input data (dict or DataFrame)
            task_name: Task identifier
            top_k: Number of top predictions to return
            
        Returns:
            Dictionary containing predictions and explanations
        """
        if not self.is_trained or task_name not in self.models:
            return {"error": "Model not trained. Please train the model first."}
        
        try:
            # Convert to DataFrame if needed
            if isinstance(user_data, dict):
                user_df = pd.DataFrame([user_data])
            else:
                user_df = user_data.copy()
            
            # Store original values for explanation
            user_original_values = user_df.copy()
            
            # Preprocess and prepare features
            user_processed = self.preprocess_data(user_df.copy())
            X_user, _ = self.prepare_features(user_processed.copy(), is_fitting=False, task_name=task_name)
            
            # Make predictions
            model_info = self.models[task_name]
            model = model_info['model']
            
            probabilities = model.predict_proba(X_user)[0]
            class_indices = np.argsort(probabilities)[::-1][:top_k]
            
            # Generate predictions with explanations
            predictions = []
            for i, class_idx in enumerate(class_indices):
                destination = self.target_encoders[task_name].classes_[class_idx]
                probability = probabilities[class_idx]
                
                # Get SHAP explanation
                shap_data = self._get_shap_explanation_data(
                    X_user, user_original_values, class_idx, task_name, probability
                )
                
                # Generate narrative explanation
                explanation = self._generate_deep_narrative(destination, shap_data)
                
                predictions.append({
                    "rank": i + 1,
                    "destination": destination,
                    "probability": round(float(probability), 4),
                    "confidence": "High" if probability > 0.7 else "Medium" if probability > 0.4 else "Low",
                    "explanation": explanation,
                    "shap_details": shap_data
                })
            
            return {
                "status": "success",
                "predictions": predictions,
                "model_info": {
                    "name": model_info['name'],
                    "test_accuracy": model_info['test_accuracy']
                },
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {"error": f"Prediction failed: {str(e)}"}

    def save_model(self, filepath: str = 'travel_destination_model.pkl', task_name: str = 'destination_prediction') -> bool:
        """
        Save the trained model and preprocessing components.
        
        Args:
            filepath: Path to save the model
            task_name: Task identifier
            
        Returns:
            True if successful, False otherwise
        """
        if not self.is_trained or task_name not in self.models:
            print("Error: No trained model to save")
            return False
        
        try:
            model_package = {
                'model': self.models[task_name],
                'label_encoders': self.label_encoders.get(task_name, {}),
                'scalers': self.scalers.get(task_name, {}),
                'feature_names': self.feature_names.get(task_name, []),
                'target_encoders': self.target_encoders.get(task_name, None),
                'shap_explainer': self.shap_explainer,
                'is_trained': self.is_trained,
                'save_timestamp': datetime.now().isoformat()
            }
            
            joblib.dump(model_package, filepath)
            print(f"Model saved successfully to: {filepath}")
            return True
            
        except Exception as e:
            print(f"Error saving model: {e}")
            return False

    def load_model(self, filepath: str = 'travel_destination_model.pkl', task_name: str = 'destination_prediction') -> bool:
        """
        Load a previously saved model.
        
        Args:
            filepath: Path to the saved model
            task_name: Task identifier
            
        Returns:
            True if successful, False otherwise
        """
        try:
            if not os.path.exists(filepath):
                print(f"Error: Model file '{filepath}' not found")
                return False
            
            model_package = joblib.load(filepath)
            
            # Restore model components
            self.models[task_name] = model_package['model']
            self.label_encoders[task_name] = model_package['label_encoders']
            self.scalers[task_name] = model_package['scalers']
            self.feature_names[task_name] = model_package['feature_names']
            self.target_encoders[task_name] = model_package['target_encoders']
            self.shap_explainer = model_package.get('shap_explainer')
            self.is_trained = model_package['is_trained']
            
            print(f"Model loaded successfully from: {filepath}")
            print(f"Model info: {self.models[task_name]['name']}")
            print(f"Test accuracy: {self.models[task_name]['test_accuracy']:.4f}")
            return True
            
        except Exception as e:
            print(f"Error loading model: {e}")
            return False

    def get_model_info(self, task_name: str = 'destination_prediction') -> Dict[str, Any]:
        """
        Get information about the trained model.
        
        Args:
            task_name: Task identifier
            
        Returns:
            Dictionary containing model information
        """
        if not self.is_trained or task_name not in self.models:
            return {"error": "No trained model available"}
        
        model_info = self.models[task_name]
        
        return {
            "model_name": model_info['name'],
            "test_accuracy": model_info['test_accuracy'],
            "train_accuracy": model_info['train_accuracy'],
            "best_parameters": model_info.get('best_params', {}),
            "feature_count": len(self.feature_names.get(task_name, [])),
            "unique_destinations": len(self.target_encoders[task_name].classes_) if task_name in self.target_encoders else 0,
            "destinations": list(self.target_encoders[task_name].classes_) if task_name in self.target_encoders else [],
            "features": self.feature_names.get(task_name, []),
            "is_trained": self.is_trained,
            "shap_available": self.shap_explainer is not None,
            "full_test_samples": len(self.X_test_full) if self.X_test_full is not None else 0,
            "shap_test_samples": len(self.X_test_shap) if self.X_test_shap is not None else 0
        }

    def generate_sample_input(self) -> Dict[str, Any]:
        """
        Generate a sample input dictionary for testing predictions.
        
        Returns:
            Dictionary with sample user data
        """
        return {
            'Traveler age': 32,
            'Destination type': 'City',
            'Accommodation type': 'Hotel',
            'Accommodation cost': 1200,
            'Transportation type': 'Flight',
            'Transportation cost': 800,
            'Duration (days)': 7,
            'Traveler gender': 'Female',
            'Traveler nationality': 'American'
        }

    def test_shap_visualizations(self):
        """Test all SHAP visualization methods."""
        if not self.is_trained:
            print("Please train the model first")
            return
        
        print("Testing SHAP visualizations...")
        
        # Get available features
        features = self.feature_names.get('destination_prediction', [])
        if not features:
            print("No features available")
            return
        
        print(f"Available features: {features[:5]}...")  # Show first 5
        
        # Test dependence plot with first available feature
        if features:
            print(f"\n1. Testing dependence plot with feature: {features[0]}")
            try:
                self.generate_dependence_plot(features[0])
            except Exception as e:
                print(f"Dependence plot failed: {e}")
        
        # Test decision plot
        print("\n2. Testing decision plot...")
        try:
            self.generate_decision_plot(max_samples=10)
        except Exception as e:
            print(f"Decision plot failed: {e}")
        
        # Test summary plots
        print("\n3. Testing summary plots...")
        try:
            self.generate_summary_plots()
        except Exception as e:
            print(f"Summary plots failed: {e}")
        
        # Test waterfall plot
        print("\n4. Testing waterfall plot...")
        try:
            self.generate_waterfall_plot(sample_index=0)
        except Exception as e:
            print(f"Waterfall plot failed: {e}")

    def train_decision_tree_for_visualization(self, max_depth: int = 4, task_name: str = 'destination_prediction') -> None:
        """
        FIXED: Train a decision tree specifically for visualization with better data handling.
        """
        if not self.is_trained:
            print("‚ùå Error: Main model must be trained first")
            return
        
        if self.X_test_full is None or self.y_test_full is None:
            print("‚ùå Error: No test data available")
            return
        
        try:
            print("üîÑ Training visualization decision tree...")
            
            # Use a good-sized sample from the full test set
            sample_size = min(2000, len(self.X_test_full))
            print(f"üìä Using {sample_size} samples from {len(self.X_test_full)} available")
            
            # Create balanced sample across destinations
            y_full = self.y_test_full
            unique_classes = np.unique(y_full)
            print(f"üìç Covering {len(unique_classes)} destinations")
            
            # Sample data
            if sample_size < len(self.X_test_full):
                sample_indices = np.random.choice(len(self.X_test_full), sample_size, replace=False)
                X_sample = self.X_test_full.iloc[sample_indices]
                y_sample = y_full[sample_indices]
            else:
                X_sample = self.X_test_full
                y_sample = y_full
            
            print(f"‚úÖ Sample prepared: {len(X_sample)} samples")
            
            # Create and train decision tree
            self.viz_tree = DecisionTreeClassifier(
                max_depth=max_depth,
                min_samples_split=20,
                min_samples_leaf=10,
                max_features='sqrt',
                class_weight='balanced',
                random_state=42
            )
            
            print("üéØ Training tree...")
            self.viz_tree.fit(X_sample, y_sample)
            
            # Validate the tree
            accuracy = self.viz_tree.score(X_sample, y_sample)
            n_features_used = np.sum(self.viz_tree.feature_importances_ > 0)
            
            print(f"‚úÖ Visualization tree trained successfully:")
            print(f"  ‚Ä¢ Training accuracy: {accuracy:.3f}")
            print(f"  ‚Ä¢ Max depth achieved: {self.viz_tree.tree_.max_depth}")
            print(f"  ‚Ä¢ Total nodes: {self.viz_tree.tree_.node_count}")
            print(f"  ‚Ä¢ Features used: {n_features_used}/{len(self.feature_names[task_name])}")
            print(f"  ‚Ä¢ Leaf nodes: {self.viz_tree.get_n_leaves()}")
            
            # Show top features being used
            feature_names = self.feature_names[task_name]
            feature_importance = self.viz_tree.feature_importances_
            
            used_features = [(feature_names[i], importance) 
                            for i, importance in enumerate(feature_importance) 
                            if importance > 0.01]
            used_features.sort(key=lambda x: x[1], reverse=True)
            
            print(f"üìà Top features in tree:")
            for i, (feature, importance) in enumerate(used_features[:5]):
                print(f"  {i+1}. {feature}: {importance:.3f}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error training visualization tree: {e}")
            import traceback
            traceback.print_exc()
            return False

    def generate_feature_usage_analysis(self, task_name: str = 'destination_prediction') -> None:
        """
        Analyze which features the XGBoost model actually uses vs the decision tree.
        """
        if not self.is_trained:
            print("Error: Model must be trained first")
            return
        
        # Get XGBoost feature importance
        xgb_model = self.models[task_name]['model']
        if hasattr(xgb_model.named_steps['classifier'], 'feature_importances_'):
            xgb_importance = xgb_model.named_steps['classifier'].feature_importances_
        else:
            print("XGBoost feature importance not available")
            return
        
        feature_names = self.feature_names[task_name]
        
        print("\n" + "="*60)
        print("FEATURE USAGE COMPARISON: XGBoost vs Decision Tree")
        print("="*60)
        
        # XGBoost top features
        xgb_features = [(feature_names[i], importance) 
                    for i, importance in enumerate(xgb_importance)]
        xgb_features.sort(key=lambda x: x[1], reverse=True)
        
        print("\nXGBoost Model - Top 10 Features:")
        for i, (feature, importance) in enumerate(xgb_features[:10]):
            print(f"  {i+1:2d}. {feature:<25} | Importance: {importance:.4f}")
        
        # Decision tree features (if available)
        if hasattr(self, 'viz_tree'):
            tree_importance = self.viz_tree.feature_importances_
            tree_features = [(feature_names[i], importance) 
                            for i, importance in enumerate(tree_importance)
                            if importance > 0]
            tree_features.sort(key=lambda x: x[1], reverse=True)
            
            print(f"\nDecision Tree - Features Used ({len(tree_features)} total):")
            for i, (feature, importance) in enumerate(tree_features):
                print(f"  {i+1:2d}. {feature:<25} | Importance: {importance:.4f}")
        
        print(f"\nüí° Key Insights:")
        print(f"‚Ä¢ XGBoost uses ALL {len(feature_names)} features in complex combinations")
        print(f"‚Ä¢ Decision tree uses subset of features for interpretability")
        print(f"‚Ä¢ Both models should consider cost, duration, age, etc.")
        print(f"‚Ä¢ For complete feature analysis, use SHAP explanations")

    def generate_decision_tree_plot(self, save_path: str = None, figsize: tuple = (35, 25)) -> None:
        """
        COMPLETELY FIXED: Generate properly separated decision tree visualization.
        This version includes comprehensive error handling and debugging.
        """
        if not self.is_trained:
            print("‚ùå Error: Model must be trained first")
            return
        
        # Check if we have test data
        if self.X_test_full is None or self.y_test_full is None:
            print("‚ùå Error: No test data available. Please retrain the model.")
            return
        
        # Train visualization tree if not exists
        if not hasattr(self, 'viz_tree'):
            print("üîÑ Training decision tree for visualization...")
            try:
                self.train_decision_tree_for_visualization(max_depth=4)
            except Exception as e:
                print(f"‚ùå Failed to train visualization tree: {e}")
                return
        
        if not hasattr(self, 'viz_tree'):
            print("‚ùå Error: Could not create visualization tree")
            return
        
        try:
            print("üé® Generating decision tree visualizations...")
            
            # Get feature and class names
            feature_names = self.feature_names.get('destination_prediction', [])
            class_names = self.target_encoders['destination_prediction'].classes_
            
            print(f"üìä Tree Info:")
            print(f"  ‚Ä¢ Features: {len(feature_names)}")
            print(f"  ‚Ä¢ Classes: {len(class_names)}")
            print(f"  ‚Ä¢ Tree depth: {self.viz_tree.tree_.max_depth}")
            print(f"  ‚Ä¢ Tree nodes: {self.viz_tree.tree_.node_count}")
            
            # SOLUTION 1: TOP LEVELS ONLY (Most Readable)
            print("üéØ Creating top-level tree visualization...")
            plt.figure(figsize=(20, 12))
            
            plot_tree(self.viz_tree,
                     feature_names=feature_names,
                     class_names=class_names,
                     filled=True,
                     rounded=True,
                     fontsize=10,
                     proportion=True,
                     impurity=False,
                     precision=2,
                     max_depth=2)  # ONLY SHOW TOP 2 LEVELS
            
            plt.title('Decision Tree - Top 2 Levels (Clear View)', 
                     fontsize=18, fontweight='bold', pad=20)
            plt.tight_layout()
            
            if save_path:
                top_path = save_path.replace('.png', '_top2.png')
                plt.savefig(top_path, dpi=300, bbox_inches='tight', 
                           facecolor='white', edgecolor='none')
                print(f"‚úÖ Top 2 levels saved to: {top_path}")
            
            plt.show()
            print("‚úÖ Top-level tree displayed successfully!")
            
            # SOLUTION 2: DEPTH 3 WITH BETTER LAYOUT
            print("üéØ Creating depth-3 tree visualization...")
            plt.figure(figsize=(30, 20))
            
            plot_tree(self.viz_tree,
                     feature_names=feature_names,
                     class_names=class_names,
                     filled=True,
                     rounded=True,
                     fontsize=8,
                     proportion=True,
                     impurity=False,
                     precision=2,
                     max_depth=3)  # Show 3 levels
            
            plt.title('Decision Tree - Depth 3 (Detailed View)', 
                     fontsize=18, fontweight='bold', pad=20)
            plt.tight_layout()
            
            if save_path:
                depth3_path = save_path.replace('.png', '_depth3.png')
                plt.savefig(depth3_path, dpi=250, bbox_inches='tight',
                           facecolor='white', edgecolor='none')
                print(f"‚úÖ Depth-3 tree saved to: {depth3_path}")
            
            plt.show()
            print("‚úÖ Depth-3 tree displayed successfully!")
            
            # SOLUTION 3: FULL TREE WITH MASSIVE CANVAS
            print("üéØ Creating full tree visualization...")
            plt.figure(figsize=(50, 35))  # MASSIVE figure
            
            plot_tree(self.viz_tree,
                     feature_names=feature_names,
                     class_names=class_names,
                     filled=True,
                     rounded=True,
                     fontsize=6,     # Very small font
                     proportion=False, # Remove proportions to save space
                     impurity=False,   # Remove impurity to save space
                     precision=1,      # Less precision
                     max_depth=4)      # Show all trained levels
            
            plt.title('Complete Decision Tree (Full Detail)', 
                     fontsize=24, fontweight='bold', pad=30)
            plt.tight_layout()
            
            if save_path:
                full_path = save_path.replace('.png', '_full.png')
                plt.savefig(full_path, dpi=150, bbox_inches='tight',
                           facecolor='white', edgecolor='none')
                print(f"‚úÖ Full tree saved to: {full_path}")
            
            plt.show()
            print("‚úÖ Full tree displayed successfully!")
            
            # SOLUTION 4: TEXT-BASED RULES (Always Works)
            print("üìù Generating text-based decision rules...")
            self.generate_text_rules()
            
            print("üéâ All decision tree visualizations completed successfully!")
            
        except Exception as e:
            print(f"‚ùå Error generating tree visualizations: {e}")
            import traceback
            traceback.print_exc()
            
            # Fallback: Try basic tree without styling
            print("üîÑ Attempting basic tree visualization...")
            try:
                plt.figure(figsize=(25, 15))
                plot_tree(self.viz_tree, max_depth=2, fontsize=8)
                plt.title('Basic Decision Tree (Fallback)')
                plt.tight_layout()
                plt.show()
                print("‚úÖ Basic tree visualization successful!")
            except Exception as fe:
                print(f"‚ùå Even basic tree visualization failed: {fe}")

    def generate_text_rules(self) -> None:
        """
        FIXED: Generate clean, readable text-based decision rules.
        """
        if not hasattr(self, 'viz_tree'):
            print("‚ùå No decision tree available")
            return
        
        try:
            print("\n" + "="*60)
            print("üìã DECISION TREE RULES (Text Format)")
            print("="*60)
            
            feature_names = self.feature_names.get('destination_prediction', [])
            class_names = self.target_encoders['destination_prediction'].classes_
            
            # Export basic tree structure
            from sklearn.tree import export_text
            tree_rules = export_text(self.viz_tree, 
                                    feature_names=feature_names,
                                    max_depth=3,
                                    spacing=3,
                                    decimals=2,
                                    show_weights=True)
            
            print(tree_rules)
            
            # Generate simplified rules
            print("\n" + "="*60)
            print("üìù SIMPLIFIED DECISION PATHS")
            print("="*60)
            
            tree = self.viz_tree.tree_
            
            def print_rules(node_id, depth=0, condition="", max_depth=3):
                if depth > max_depth:
                    return
                
                indent = "  " * depth
                
                # Check if leaf node
                if tree.children_left[node_id] == tree.children_right[node_id]:
                    # Leaf node - show prediction
                    class_counts = tree.value[node_id][0]
                    predicted_class_id = np.argmax(class_counts)
                    predicted_class = class_names[predicted_class_id]
                    confidence = class_counts[predicted_class_id] / np.sum(class_counts)
                    samples = int(tree.n_node_samples[node_id])
                    
                    print(f"{indent}üéØ PREDICT: {predicted_class}")
                    print(f"{indent}   Confidence: {confidence:.2%} ({samples} samples)")
                    return
                
                # Internal node - show split
                feature_id = tree.feature[node_id]
                feature_name = feature_names[feature_id]
                threshold = tree.threshold[node_id]
                samples = int(tree.n_node_samples[node_id])
                
                print(f"{indent}‚ùì IF {feature_name} ‚â§ {threshold:.2f} ({samples} samples):")
                print_rules(tree.children_left[node_id], depth + 1, 
                           f"{condition} AND {feature_name} ‚â§ {threshold:.2f}" if condition else f"{feature_name} ‚â§ {threshold:.2f}")
                
                print(f"{indent}‚ùì ELSE {feature_name} > {threshold:.2f}:")
                print_rules(tree.children_right[node_id], depth + 1,
                           f"{condition} AND {feature_name} > {threshold:.2f}" if condition else f"{feature_name} > {threshold:.2f}")
            
            print_rules(0)
            
            print("\n" + "="*60)
            print("‚úÖ Decision tree rules generated successfully!")
            print("="*60)
            
        except Exception as e:
            print(f"‚ùå Error generating text rules: {e}")

    def generate_simplified_tree_rules(self, max_rules: int = 10) -> None:
        """
        Generate human-readable decision tree rules.
        
        Args:
            max_rules: Maximum number of rules to display
        """
        if not hasattr(self, 'viz_tree'):
            print("Decision tree not available. Please generate tree plot first.")
            return
        
        try:
            feature_names = self.feature_names.get('destination_prediction', [])
            class_names = self.target_encoders['destination_prediction'].classes_
            
            # Export tree rules as text
            tree_rules = export_text(self.viz_tree, 
                                feature_names=feature_names,
                                max_depth=4,
                                spacing=2,
                                decimals=2)
            
            print("Decision Tree Rules:")
            print("="*50)
            print(tree_rules)
            
            # Also create simplified rules
            print("\nSimplified Decision Rules (Top Paths):")
            print("="*50)
            
            tree = self.viz_tree.tree_
            feature_names_array = np.array(feature_names)
            
            def get_rules(node_id, depth=0, condition=""):
                if depth > 3:  # Limit depth for readability
                    return
                
                if tree.children_left[node_id] == tree.children_right[node_id]:  # Leaf node
                    class_id = np.argmax(tree.value[node_id][0])
                    class_name = class_names[class_id]
                    samples = int(tree.n_node_samples[node_id])
                    print(f"{'  ' * depth}‚Üí Predict: {class_name} ({samples} samples)")
                    return
                
                feature = feature_names_array[tree.feature[node_id]]
                threshold = tree.threshold[node_id]
                
                # Left child (feature <= threshold)
                left_condition = f"{condition} AND {feature} ‚â§ {threshold:.2f}" if condition else f"{feature} ‚â§ {threshold:.2f}"
                print(f"{'  ' * depth}If {feature} ‚â§ {threshold:.2f}:")
                get_rules(tree.children_left[node_id], depth + 1, left_condition)
                
                # Right child (feature > threshold)
                right_condition = f"{condition} AND {feature} > {threshold:.2f}" if condition else f"{feature} > {threshold:.2f}"
                print(f"{'  ' * depth}Else ({feature} > {threshold:.2f}):")
                get_rules(tree.children_right[node_id], depth + 1, right_condition)
            
            get_rules(0)
            
        except Exception as e:
            print(f"Error generating tree rules: {e}")

    def test_tree_visualization(self) -> None:
        """
        FIXED: Test the decision tree visualization with comprehensive debugging.
        """
        print("üß™ Testing decision tree visualization...")
        
        if not self.is_trained:
            print("‚ùå Model not trained")
            return
        
        print(f"‚úÖ Model is trained")
        print(f"üìä Test data available: {self.X_test_full is not None}")
        print(f"üìä Test data size: {len(self.X_test_full) if self.X_test_full is not None else 0}")
        
        # Test tree training
        try:
            if not hasattr(self, 'viz_tree'):
                print("üîÑ Training visualization tree...")
                self.train_decision_tree_for_visualization()
            
            if hasattr(self, 'viz_tree'):
                print("‚úÖ Visualization tree available")
                print(f"  ‚Ä¢ Tree depth: {self.viz_tree.tree_.max_depth}")
                print(f"  ‚Ä¢ Tree nodes: {self.viz_tree.tree_.node_count}")
            else:
                print("‚ùå Visualization tree not available")
                return
        except Exception as e:
            print(f"‚ùå Tree training failed: {e}")
            return
        
        # Test visualization
        try:
            print("üé® Testing tree visualization...")
            self.generate_decision_tree_plot(save_path="test_tree.png")
        except Exception as e:
            print(f"‚ùå Tree visualization failed: {e}")
            import traceback
            traceback.print_exc()

    def generate_tree_performance_comparison(self, save_path: str = None) -> None:
        """
        Compare the performance of the main XGBoost model vs the visualization tree.
        """
        if not self.is_trained or not hasattr(self, 'viz_tree'):
            print("Both main model and visualization tree must be available")
            return
        
        try:
            # Use full test data for comparison
            X_test = self.X_test_full
            y_test = self.y_test_full
            
            # Get predictions from both models
            main_model = self.models['destination_prediction']['model']
            main_pred = main_model.predict(X_test)
            tree_pred = self.viz_tree.predict(X_test)
            
            # Calculate accuracies
            main_accuracy = np.mean(main_pred == y_test)
            tree_accuracy = np.mean(tree_pred == y_test)
            
            print("\nModel Performance Comparison:")
            print("="*40)
            print(f"XGBoost Model Accuracy: {main_accuracy:.4f}")
            print(f"Decision Tree Accuracy: {tree_accuracy:.4f}")
            print(f"Accuracy Difference: {main_accuracy - tree_accuracy:.4f}")
            
            # Create comparison plot
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            models = ['XGBoost', 'Decision Tree']
            accuracies = [main_accuracy, tree_accuracy]
            colors = ['#2E86AB', '#A23B72']
            
            bars = ax1.bar(models, accuracies, color=colors, alpha=0.7)
            ax1.set_ylabel('Accuracy')
            ax1.set_title('Model Accuracy Comparison')
            ax1.set_ylim(0, 1)
            
            # Add value labels on bars
            for bar, acc in zip(bars, accuracies):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
            
            # Feature importance comparison (top 5 features)
            if hasattr(main_model.named_steps['classifier'], 'feature_importances_'):
                main_importance = main_model.named_steps['classifier'].feature_importances_
            else:
                main_importance = np.zeros(len(self.feature_names['destination_prediction']))
            
            tree_importance = self.viz_tree.feature_importances_
            feature_names = self.feature_names['destination_prediction']
            
            # Get top 5 features from XGBoost
            top_indices = np.argsort(main_importance)[-5:][::-1]
            
            x_pos = np.arange(len(top_indices))
            width = 0.35
            
            ax2.bar(x_pos - width/2, main_importance[top_indices], width, 
                label='XGBoost', color=colors[0], alpha=0.7)
            ax2.bar(x_pos + width/2, tree_importance[top_indices], width,
                label='Decision Tree', color=colors[1], alpha=0.7)
            
            ax2.set_xlabel('Features')
            ax2.set_ylabel('Importance')
            ax2.set_title('Feature Importance Comparison (Top 5)')
            ax2.set_xticks(x_pos)
            ax2.set_xticklabels([feature_names[i][:15] + '...' if len(feature_names[i]) > 15 
                                else feature_names[i] for i in top_indices], rotation=45)
            ax2.legend()
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"Performance comparison saved to: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"Error in performance comparison: {e}")

    def generate_all_tree_visualizations(self, output_dir: str = "tree_visualizations") -> None:
        """
        Generate all decision tree visualizations.
        
        Args:
            output_dir: Directory to save all tree visualizations
        """
        if not self.is_trained:
            print("Error: Model must be trained first")
            return
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        print("Generating decision tree visualizations...")
        
        # 1. Decision Tree Plot
        try:
            print("Generating decision tree plot...")
            self.generate_decision_tree_plot(
                save_path=os.path.join(output_dir, "decision_tree_visualization.png")
            )
            print("‚úì Decision tree plot generated")
        except Exception as e:
            print(f"‚úó Error generating decision tree plot: {e}")
        
        # 2. Tree Rules
        try:
            print("Generating decision tree rules...")
            self.generate_simplified_tree_rules()
            print("‚úì Decision tree rules generated")
        except Exception as e:
            print(f"‚úó Error generating tree rules: {e}")
        
        # 3. Performance Comparison
        try:
            print("Generating performance comparison...")
            self.generate_tree_performance_comparison(
                save_path=os.path.join(output_dir, "model_comparison.png")
            )
            print("‚úì Performance comparison generated")
        except Exception as e:
            print(f"‚úó Error generating performance comparison: {e}")
        
        print(f"Decision tree visualizations saved to: {output_dir}")

    def quick_tree_test(self):
        """Quick test to see what's happening with the tree."""
        print("üß™ QUICK TREE DIAGNOSTIC:")
        print(f"‚úÖ Model trained: {self.is_trained}")
        print(f"‚úÖ Test data available: {self.X_test_full is not None}")
        if self.X_test_full is not None:
            print(f"‚úÖ Test data size: {len(self.X_test_full)}")
        print(f"‚úÖ Viz tree exists: {hasattr(self, 'viz_tree')}")
        
        # Try to create a simple tree
        if self.is_trained and self.X_test_full is not None:
            try:
                print("üîÑ Creating simple tree...")
                from sklearn.tree import DecisionTreeClassifier
                simple_tree = DecisionTreeClassifier(max_depth=2, random_state=42)
                
                # Use small sample
                X_small = self.X_test_full.sample(n=min(500, len(self.X_test_full)))
                y_small = self.y_test_full[:len(X_small)]
                
                simple_tree.fit(X_small, y_small)
                print("‚úÖ Simple tree trained successfully!")
                
                # Try basic plot
                import matplotlib.pyplot as plt
                from sklearn.tree import plot_tree
                
                plt.figure(figsize=(15, 10))
                plot_tree(simple_tree, max_depth=2, fontsize=10, filled=True)
                plt.title("Simple Test Tree")
                plt.show()
                print("‚úÖ Simple tree displayed!")
                
            except Exception as e:
                print(f"‚ùå Simple tree failed: {e}")
                import traceback
                traceback.print_exc()



def main():
    """
    Main function to demonstrate the travel destination predictor with bias analysis.
    """
    # Initialize the predictor
    predictor = TravelDestinationPredictor()
    
    print("=== Travel Destination Predictor with Bias Analysis ===\n")
    
    # Step 1: Train the model
    try:
        print("1. Training the model...")
        training_results = predictor.train_model('extended_travel_trips.csv')
        print(f"‚úì Training completed with test accuracy: {training_results['test_accuracy']}")
        print(f"‚úì Full test samples: {training_results['test_samples']}")
        print(f"‚úì SHAP samples: {training_results['test_samples_shap']}")
        print()
    except FileNotFoundError:
        print("‚ùå Training data file not found. Please ensure 'extended_travel_trips.csv' exists.")
        print("You can download sample data or create your own CSV file.")
        return
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        return
    
    # Step 2: Generate confusion matrix
    print("2. Generating confusion matrix with full test dataset...")
    try:
        predictor.generate_confusion_matrix(save_path="confusion_matrix.png")
        print("‚úÖ Confusion matrix generated successfully using full test set!\n")
    except Exception as e:
        print(f"‚ùå Confusion matrix generation failed: {e}\n")
    
    # Step 3: Generate model performance visualizations
    print("3. Generating comprehensive model performance visualizations...")
    try:
        predictor.generate_model_performance_plots("performance_output")
        print("‚úÖ Model performance visualizations generated successfully!\n")
    except Exception as e:
        print(f"‚ùå Performance visualization generation failed: {e}\n")
    
    # Step 4: NEW - Generate bias analysis (ADD THIS SECTION)
    print("4. Generating comprehensive bias analysis...")
    try:
        bias_results = predictor.analyze_model_bias(output_dir='bias_analysis')
        print("‚úÖ Bias analysis completed successfully!\n")
        
        # Print summary of bias findings
        if bias_results:
            print("üìä Bias Analysis Summary:")
            for bias_type, results in bias_results.items():
                if results:
                    print(f"  ‚Ä¢ {bias_type.title()}: {len(results)} groups analyzed")
            print()
    except Exception as e:
        print(f"‚ùå Bias analysis failed: {e}\n")
    
    # Step 5: Test SHAP visualizations (renumbered)
    print("5. Testing SHAP visualizations...")
    try:
        predictor.test_shap_visualizations()
        print("‚úÖ SHAP visualizations tested successfully!\n")
    except Exception as e:
        print(f"‚ùå SHAP visualization testing failed: {e}\n")
    
    # Step 6: Generate all SHAP visualizations (renumbered)
    print("6. Generating comprehensive SHAP visualizations...")
    try:
        predictor.generate_all_shap_plots("shap_output")
        print("‚úÖ All SHAP visualizations generated successfully!\n")
    except Exception as e:
        print(f"‚ùå Complete SHAP visualization generation failed: {e}\n")
    
    # Step 7: Make sample predictions with explanations (renumbered)
    print("7. Making sample predictions with SHAP explanations...")
    sample_data = predictor.generate_sample_input()
    print(f"Sample input: {sample_data}")
    
    predictions = predictor.predict_destination(sample_data, top_k=3)
    
    if predictions.get("status") == "success":
        print("\nüìç Top 3 Destination Predictions:")
        for pred in predictions["predictions"]:
            print(f"\n{pred['rank']}. {pred['destination']}")
            print(f"   Probability: {pred['probability']:.3f} ({pred['confidence']} confidence)")
            print(f"   Explanation: {pred['explanation']}")
    else:
        print(f"‚ùå Prediction failed: {predictions.get('error')}")
    
    # Step 8: Test with custom user input (renumbered)
    print("\n8. Testing with custom user input...")
    try:
        custom_input = {
            'Traveler DOB': '10/08/1996', 
            'Season': 'Summer', 
            'Duration (days)': 7, 
            'Traveler gender': 'Female', 
            'Traveler nationality': 'American', 
            'Accommodation type': 'Hotel', 
            'Accommodation cost': 1200, 
            'Transportation type': 'Flight', 
            'Transportation cost': 800
        }
        
        print(f"Custom input: {custom_input}")
        custom_predictions = predictor.predict_destination(custom_input, top_k=2)
        
        if custom_predictions.get("status") == "success":
            print("\nüéØ Top 2 Predictions for Custom Input:")
            for pred in custom_predictions["predictions"]:
                print(f"\n{pred['rank']}. {pred['destination']}")
                print(f"   Probability: {pred['probability']:.3f}")
                print(f"   Explanation: {pred['explanation']}")
        else:
            print(f"‚ùå Custom prediction failed: {custom_predictions.get('error')}")
    except Exception as e:
        print(f"‚ùå Custom prediction testing failed: {e}")
    
    # Step 9: Save the trained model (renumbered)
    print("\n9. Saving the trained model...")
    try:
        if predictor.save_model('travel_destination_model.pkl'):
            print("‚úÖ Model saved successfully!")
        else:
            print("‚ùå Model saving failed!")
    except Exception as e:
        print(f"‚ùå Model saving failed: {e}")
    
    # Step 10: Display comprehensive model information (renumbered)
    print("\n10. Model Information Summary:")
    try:
        model_info = predictor.get_model_info()
        print(f"üìä Model Performance:")
        print(f"  ‚Ä¢ Model Name: {model_info.get('model_name')}")
        print(f"  ‚Ä¢ Test Accuracy: {model_info.get('test_accuracy'):.4f}")
        print(f"  ‚Ä¢ Train Accuracy: {model_info.get('train_accuracy'):.4f}")
        print(f"  ‚Ä¢ Number of Features: {model_info.get('feature_count')}")
        print(f"  ‚Ä¢ Number of Destinations: {model_info.get('unique_destinations')}")
        print(f"  ‚Ä¢ Full Test Samples: {model_info.get('full_test_samples')} (for confusion matrix)")
        print(f"  ‚Ä¢ SHAP Test Samples: {model_info.get('shap_test_samples')} (for visualizations)")
        print(f"  ‚Ä¢ SHAP Available: {model_info.get('shap_available')}")
        print(f"  ‚Ä¢ Available Destinations: {', '.join(model_info.get('destinations', [])[:5])}...")
        print(f"  ‚Ä¢ Key Features: {', '.join(model_info.get('features', [])[:5])}...")
    except Exception as e:
        print(f"‚ùå Model information retrieval failed: {e}")
    
    # Step 11: Generate decision tree visualizations (renumbered)
    print("\n11. Generating decision tree visualizations...")
    try:
        predictor.generate_all_tree_visualizations("tree_output")
        print("‚úÖ Decision tree visualizations completed!\n")
    except Exception as e:
        print(f"‚ùå Decision tree visualization failed: {e}\n")
    
    # Step 12: Generate additional specific visualizations (renumbered)
    print("12. Generating additional specific visualizations...")
    try:
        # Generate dependence plot for a specific feature
        if predictor.feature_names.get('destination_prediction'):
            features = predictor.feature_names['destination_prediction']
            if 'Age' in features:
                print("Generating Age dependence plot...")
                predictor.generate_dependence_plot('Age', save_path="age_dependence_plot.png")
            
            if 'Duration (days)' in features:
                print("Generating Duration dependence plot...")
                predictor.generate_dependence_plot('Duration (days)', save_path="duration_dependence_plot.png")
        
        print("‚úÖ Additional visualizations completed!")
    except Exception as e:
        print(f"‚ùå Additional visualizations failed: {e}")


    # Step 13: Final summary and file locations (renumbered)
    print("\n" + "="*70)
    print("üéâ EXECUTION SUMMARY - WITH BIAS ANALYSIS!")
    print("="*70)
    print("üìÅ Generated Files:")
    print("‚Ä¢ confusion_matrix.png - Confusion matrix using full test set")
    print("‚Ä¢ performance_output/ - Model performance plots")
    print("‚Ä¢ bias_analysis/ - üÜï BIAS ANALYSIS PLOTS (demographic, economic, geographic)")
    print("‚Ä¢ shap_output/ - All SHAP visualizations")
    print("‚Ä¢ tree_output/ - Decision tree visualizations")
    print("‚Ä¢ travel_destination_model.pkl - Saved trained model")
    print("‚Ä¢ age_dependence_plot.png - Age feature analysis")
    print("‚Ä¢ duration_dependence_plot.png - Duration feature analysis")
    
    print(f"\nüìà Model Performance:")
    if 'training_results' in locals():
        print(f"‚Ä¢ Test Accuracy: {training_results.get('test_accuracy', 'N/A')}")
        print(f"‚Ä¢ Training Samples: {training_results.get('training_samples', 'N/A')}")
        print(f"‚Ä¢ Full Test Samples: {training_results.get('test_samples', 'N/A')}")
        print(f"‚Ä¢ SHAP Test Samples: {training_results.get('test_samples_shap', 'N/A')}")
        print(f"‚Ä¢ Unique Destinations: {training_results.get('unique_destinations', 'N/A')}")
    
    print(f"\nüîß FEATURES IMPLEMENTED:")
    print(f"‚Ä¢ ‚úÖ Confusion matrix using full test dataset")
    print(f"‚Ä¢ ‚úÖ SHAP visualizations using optimized sample")
    print(f"‚Ä¢ ‚úÖ Decision tree visualizations with multiple depth levels")
    print(f"‚Ä¢ ‚úÖ Enhanced plotting with improved spacing")
    print(f"‚Ä¢ ‚úÖ üÜï COMPREHENSIVE BIAS ANALYSIS (demographic, economic, geographic)")
    print(f"‚Ä¢ ‚úÖ Model performance evaluation and comparison")
    
    print(f"\nüìä Visualization Types Generated:")
    print("‚Ä¢ ‚úÖ Confusion Matrix - Prediction accuracy per destination")
    print("‚Ä¢ ‚úÖ Decision Tree Plot - Visual tree structure")
    print("‚Ä¢ ‚úÖ Decision Tree Rules - Human-readable if-then rules")
    print("‚Ä¢ ‚úÖ Model Comparison - XGBoost vs Decision Tree performance")
    print("‚Ä¢ ‚úÖ SHAP Summary Plots - Feature importance and impact")
    print("‚Ä¢ ‚úÖ SHAP Dependence Plots - Individual feature effects")
    print("‚Ä¢ ‚úÖ SHAP Decision Plots - Model decision process")
    print("‚Ä¢ ‚úÖ SHAP Waterfall Plots - Individual prediction breakdown")
    print("‚Ä¢ ‚úÖ Classification Report - Precision, recall, F1-scores")
    print("‚Ä¢ ‚úÖ üÜï BIAS ANALYSIS PLOTS - Demographic, economic, and geographic bias detection")
    
    print(f"\nüéØ Bias Analysis Includes:")
    if 'bias_results' in locals() and bias_results:
        for bias_type, results in bias_results.items():
            if results:
                print(f"‚Ä¢ ‚úÖ {bias_type.title()} Bias: {len(results)} groups analyzed")
    else:
        print("‚Ä¢ üÜï Demographic Bias - Age, gender, nationality analysis")
        print("‚Ä¢ üÜï Economic Bias - Budget category analysis") 
        print("‚Ä¢ üÜï Geographic Bias - Continental distribution analysis")
    
    print(f"\nüéâ Complete AI model analysis with bias detection completed!")
    print(f"Check the 'bias_analysis' directory for fairness and bias evaluation results.")
    print("="*70)

def test_enhanced_predictions():
    """Test the enhanced prediction explanations with SHAP methodology"""
    print("\n" + "="*70)
    print("üß™ TESTING ENHANCED PREDICTIONS WITH REAL SHAP WEIGHTS")
    print("="*70)
    
    # Load the trained model
    predictor = TravelDestinationPredictor()
    
    try:
        success = predictor.load_model()
        if not success:
            print("‚ùå Could not load trained model")
            return
        
        print("‚úÖ Model loaded successfully!")
        
        # Test with sample user data
        sample_user = {
            'Traveler age': 28,
            'Traveler gender': 'Female', 
            'Traveler nationality': 'American',
            'Duration (days)': 7,
            'Accommodation type': 'Hotel',
            'Accommodation cost': 1200,
            'Transportation type': 'Flight',
            'Transportation cost': 800
        }
        
        print(f"\nüìã Sample User Profile:")
        for key, value in sample_user.items():
            print(f"   ‚Ä¢ {key}: {value}")
        
        print(f"\nüéØ Getting TOP 5 PREDICTIONS with Enhanced Explanations...")
        print("-" * 70)
        
        # Get predictions
        result = predictor.predict_destination(sample_user, top_k=5)
        
        if result.get('status') == 'success':
            predictions = result['predictions']
            
            print(f"\nüèÜ TOP 5 DESTINATION PREDICTIONS:")
            print(f"   Model: {result['model_info']['name']}")
            print(f"   Accuracy: {result['model_info']['test_accuracy']:.1%}")
            print(f"   Timestamp: {result['timestamp']}")
            
            # Show just the first prediction with full explanation
            if predictions:
                pred = predictions[0]
                print(f"\n" + "="*50)
                print(f"ü•á TOP PREDICTION DETAILED EXPLANATION")
                print(f"="*50)
                print(pred['explanation'])
                print(f"="*50)
                
                # Show summary of all 5 predictions
                print(f"\nüìä ALL 5 PREDICTIONS SUMMARY:")
                for i, pred in enumerate(predictions, 1):
                    conf_emoji = "üî•" if pred['probability'] > 0.7 else "‚úÖ" if pred['probability'] > 0.4 else "‚ö†Ô∏è"
                    print(f"{i}. {conf_emoji} {pred['destination']} - {pred['probability']:.1%} confidence ({pred['confidence']})")
        
        else:
            print(f"‚ùå Prediction failed: {result}")
    
    except Exception as e:
        print(f"‚ùå Error testing predictions: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
    
    # Test enhanced predictions after training
    test_enhanced_predictions()